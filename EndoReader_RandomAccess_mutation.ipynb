{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Interprets the repaired json for the endomondo data\n",
    "\n",
    "#Might want to check for the presence of all attributes\n",
    "\n",
    "#Need to provide separate training, validation, and test data\n",
    "#Need to precompute max and min of numerical variables for scaling\n",
    "#Need to impliment scaling\n",
    "\n",
    "import numpy as np\n",
    "import ijson\n",
    "import pickle\n",
    "import os\n",
    "from future import division\n",
    "\n",
    "class dataInterpreter(object):\n",
    "    \"\"\"Going to want to impliment a thing that fixes the data (attribute) schema \n",
    "    once it's set both for effeciency purposes and for safety purposes.\n",
    "    This means that this object gets a list of attributes to focus on that is immutable\n",
    "    and always retrieves data w.r.t. this attribute list.\n",
    "    This attribute list should also be readable so that programs that use this class\n",
    "    can configure things based on information about how the variables are handled in this reader.\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"The most user-relevant functions are buildDataSchema, nextBatch, and newEpoch. Use the rest with caution,\n",
    "    as they may have unanticipated effects (for someone not familiar with this package)\"\"\"\n",
    "    \n",
    "    #The dataSet paramater can be set to \"train\", \"valid\", or \"test\" to read from those data sets respectively \n",
    "    \n",
    "    def __init__(self, fn=\"endomondoHR_proper.json\", attributes=None, dataSet=\"train\", allowMissingData=True):\n",
    "        self.dataFileName=fn#Will eventually replace this with a data folder name\n",
    "        self.MetaDataLoaded=False\n",
    "        self.dataSchemaLoaded=False\n",
    "        self.currentDataPoint=None\n",
    "        self.currentDataPointNumber=0\n",
    "        self.dataPointPosition=0\n",
    "        self.attIgnore=['id','url']#Attributes to ignore when building metadata\n",
    "        self.metaDataFn=fn[0:len(fn)-5]+\"_metaData.p\"\n",
    "        self.allowMissingData=allowMissingData\n",
    "        #self.valTestSplit=(.1,.1)\n",
    "        if attributes is not None:\n",
    "            self.buildDataSchema(attributes)\n",
    "    \n",
    "    def createGenerator(self): #Define a new data generator\n",
    "        filename = self.dataFileName\n",
    "        self.f=open(filename, 'r')\n",
    "        objects = ijson.items(self.f, 'users.item')\n",
    "        self.dataObjects=objects\n",
    "        return self.dataObjects\n",
    "\n",
    "    \"\"\"def getNDataPoints(self, n):\n",
    "        try: #If there is a generator already defined\n",
    "            objects=self.dataObjects\n",
    "        except: #Otherwise create a new one\n",
    "            #Creating new generator\n",
    "            objects=self.createGenerator()     \n",
    "        columns = []\n",
    "        for i in range(0,n):\n",
    "            columns.append(objects.next())\n",
    "        return self.__convert(columns) #returns data as a list of dictionaries where each dictionary represents a particular exercise by a particular user\n",
    "    \"\"\"\n",
    "    def getNextDataPoint(self):\n",
    "        #Need to:\n",
    "        #0. Scan the file when producing metadata and create a list of data point indices\n",
    "        #1. Load the list of data point indices\n",
    "        #2. Randomly permute the list of data point indices\n",
    "        #3. Split the list of data point indices into training, validation, and test sets\n",
    "        #4. Provide a method to get the next data point in the list (and either globally save the position of the list or save it implicitly in a generator)\n",
    "        #5. Provide a method to reset this list position (for a new epoch)\n",
    "        #6. Rewrite the endoIterator method to respect randomization (copy the random access iterator code from the ptb reader example)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        try: #If there is a generator already defined\n",
    "            objects=self.dataObjects\n",
    "        except: #Otherwise create a new one\n",
    "            #Creating new generator\n",
    "            objects=self.createGenerator()\n",
    "        potentialNextDataPoint=self.__convert(objects.next())\n",
    "        if self.allowMissingData==False:\n",
    "            #Check if the next data point contains all the requested attributes\n",
    "            for i, att in enumerate(self.attributes):\n",
    "                #print(att)\n",
    "                try:\n",
    "                    test=self.currentDataPoint[att]\n",
    "                except:\n",
    "                    print(\"Skipping data point because it lacks attribute: \" + att)\n",
    "                    #print(\"Skipping data point because it lacks attribute\")\n",
    "                    return self.getNextDataPoint() #Try the next one instead\n",
    "        return potentialNextDataPoint #returns next data point\n",
    "        \"\"\"\n",
    "    \n",
    "    def __convert(self, unicData): #Converts the unicode text in a dictionary to ascii\n",
    "        #Shamelessly lifted from http://stackoverflow.com/questions/13101653/python-convert-complex-dictionary-of-strings-from-unicode-to-ascii\n",
    "        if isinstance(unicData, dict):\n",
    "            return {self.__convert(key): self.__convert(value) for key, value in unicData.iteritems()}\n",
    "        elif isinstance(unicData, list):\n",
    "            return [self.__convert(element) for element in unicData]\n",
    "        elif isinstance(unicData, unicode):\n",
    "            return unicData.encode('utf-8')\n",
    "        else:\n",
    "            return unicData\n",
    "    \n",
    "    def getDataClasses(self, data):\n",
    "        class_label_lists = [col.keys() for col in data]\n",
    "        class_labels = [ item for sublist in class_label_lists for item in sublist]\n",
    "        return np.unique(np.array(class_labels))\n",
    "        \n",
    "    def getDataLabels(self, data, dataClass):\n",
    "        #The \"data\" argument is in the same format as is returned by \"getNdatapoints\"\n",
    "        #If there is a use case that involves finding all the possible labels for a given class, a seperate function should be written to save memory usage...\n",
    "        class_labels = [col[dataClass] for col in data]\n",
    "        return np.unique(np.array(class_labels))\n",
    "    \n",
    "    def getAttributeData(self, data, dataClass):\n",
    "        #Get list of data points of a given attribute from a data thing (list of dictionaries)\n",
    "        class_labels = [col[dataClass] for col in data]\n",
    "        return class_labels\n",
    "    \n",
    "    def buildEncoder(self, classLabels):\n",
    "        #Constructs a dictionary that maps each class label to a list (encoding scheme) where one entry in the list is 1 and the remainder are 0\n",
    "        encodingLength=classLabels.size\n",
    "        encoder={}\n",
    "        for i, label in enumerate(classLabels):\n",
    "            encoding=[0] * encodingLength\n",
    "            encoding[i]=1\n",
    "            encoder[label]=encoding\n",
    "        return encoder\n",
    "    \n",
    "    def writeSummaryFile(self):\n",
    "        metaDataForWriting=metaDataEndomondo(self.numDataPoints, self.encodingLengths, self.oneHotEncoders, self.isSequence, self.isNominal)\n",
    "        with open(self.metaDataFn, \"wb\") as f:\n",
    "            pickle.dump(metaDataForWriting, f)\n",
    "\n",
    "        #pickle.dump(metaDataForWriting, open(self.metaDataFn, \"wb\"))\n",
    "        print(\"Summary file written\")\n",
    "        \n",
    "    def loadSummaryFile(self):\n",
    "        try:\n",
    "            print(\"Loading metadata\")\n",
    "            with open(self.metaDataFn, \"rb\") as f:\n",
    "                metaData = pickle.load(f)\n",
    "                \n",
    "            #metaData=pickle.load(open(self.metaDataFn, \"rb\"))\n",
    "        except:\n",
    "            raise(IOError(\"Metadata file: \" + self.metaDataFn + \" not in valid pickle format\"))\n",
    "        self.numDataPoints=metaData.numDataPoints\n",
    "        self.encodingLengths=metaData.encodingLengths\n",
    "        self.oneHotEncoders=metaData.oneHotEncoders\n",
    "        #self.dataDim=metaData.dataDim\n",
    "        self.isSequence=metaData.isSequence\n",
    "        self.isNominal=metaData.isNominal\n",
    "        print(\"Metadata loaded\")\n",
    "        \n",
    "    def buildDataSchema(self, attributes):\n",
    "        self.buildMetaData()\n",
    "        self.splitForValidation((.8,.1,.1))\n",
    "        self.newEpoch()#Reset all indices and counters\n",
    "        self.attributes=attributes\n",
    "        dataDimSum=0\n",
    "        for att in self.attributes:\n",
    "            dataDimSum=dataDimSum+self.encodingLengths[att]\n",
    "        self.dataDim=dataDimSum\n",
    "        self.dataSchemaLoaded=True \n",
    "        \n",
    "    def splitForValidation(valTestSplit):\n",
    "        #Construct seperate data files for the training, test, and validation data\n",
    "        unallocatedDataPoints=self.numDataPoints\n",
    "        trainingSetSize=self.numDataPoints*valTestSplit[0]\n",
    "        validationSetSize=self.numDataPoints*valTestSplit[1]\n",
    "        testSetSize=self.numDataPoints*valTestSplit[2]\n",
    "        \n",
    "        trainingSetAllocated=0\n",
    "        validationSetAllocated=0\n",
    "        testSetAllocated=0\n",
    "        #Allocate data points to the seperate data sets with probability proportional to the number of data points that each set still needs\n",
    "        for i in range(self.numDataPoints):\n",
    "            #Compute probabilities\n",
    "            trainProb=1-(trainingSetAllocated/float(trainingSetSize))\n",
    "            validProb=1-(validationSetAllocated/float(validationSetSize))\n",
    "            testProb=1-(testSetAllocated/float(testSetSize))\n",
    "            #Sample\n",
    "            r=random.random()\n",
    "            if r < trainProb:\n",
    "                #Assign to training set\n",
    "            elif r < validProb:\n",
    "                #Assign to validation set\n",
    "            else:\n",
    "                #Assign to test set\n",
    "            \n",
    "            #Grab data point and assign\n",
    "        \n",
    "        #Save metadata for each set (probably just need the number of data points...)\n",
    "        #Write the data files with the appropriate data points (possibly do this progressively within the for loop)\n",
    "        \n",
    "    def buildMetaData(self):\n",
    "        #Takes a list of attributes and the current datafile and constructs a schema for the data to be input into the RNN.\n",
    "        if os.path.isfile(self.metaDataFn):#If a summary file exists\n",
    "            self.loadSummaryFile()#Load that summary file and use it to capture all the necessary info\n",
    "        else:\n",
    "            print(\"Building data schema\")\n",
    "            #Build such a summary file by running through the full dataset and capturing the necessary statistics\n",
    "            self.isSequence={'altitude':True, 'gender':False, 'heart_rate':True, 'id':False, 'latitude':True, 'longitude':True,\n",
    "                             'speed':True, 'sport':False, 'timestamp':True, 'url':False, 'userId':False}#Handcoded\n",
    "            self.isNominal={'altitude':False, 'gender':True, 'heart_rate':False, 'id':True, 'latitude':False, 'longitude':False,\n",
    "                            'speed':False, 'sport':True, 'timestamp':False, 'url':True, 'userId':True}#Handcoded\n",
    "            allDataClasses=['altitude', 'gender', 'heart_rate', 'id', 'latitude', 'longitude',\n",
    "       'speed', 'sport', 'timestamp', 'url', 'userId']\n",
    "            dataClasses=[x for x in allDataClasses if x not in self.attIgnore]#get rid of the attributes that we are ignoring\n",
    "            self.newEpoch()#makes sure to reset things\n",
    "            moreData=True\n",
    "            classLabels={}\n",
    "            numDataPoints=0\n",
    "            while moreData:\n",
    "                if numDataPoints%1000==0:\n",
    "                    print(\"Currently at data point \" + str(numDataPoints))\n",
    "                try:\n",
    "                    currData=self.getNDataPoints(1)\n",
    "                    #dataClasses = self.getDataClasses(currData)#This could be removed to make it more effecient\n",
    "                    for datclass in dataClasses:\n",
    "                        if self.isNominal[datclass]: #If it is nominal data\n",
    "                            if self.isSequence[datclass]:\n",
    "                                raise(NotImplementedError(\"Nominal data types for sequences have not yet been implemented\"))\n",
    "                            dataClassLabels=self.getDataLabels(currData, datclass)\n",
    "                            if classLabels.get(datclass) is None: #If it is the first step\n",
    "                                classLabels[datclass]=dataClassLabels\n",
    "                            else:\n",
    "                                #print(np.concatenate(dataClassLabels,classLabels[datclass]))\n",
    "                                classLabels[datclass]=np.unique(np.concatenate([dataClassLabels,classLabels[datclass]]))\n",
    "                        else:\n",
    "                            if self.isSequence[datclass]!=True:\n",
    "                                #If is it nominal and not a sequence\n",
    "                                raise(NotImplementedError(\"Non-nominal data types for non-sequences have not yet been implemented\"))\n",
    "                    numDataPoints=numDataPoints+1\n",
    "                except:\n",
    "                    moreData=False\n",
    "                    print(\"Stopped at \" + str(numDataPoints) + \" data points\")\n",
    "                #if numDataPoints>10000:#For testing\n",
    "                #    moreData=False#For testing\n",
    "            \n",
    "            oneHotEncoders={}\n",
    "            encodingLengths={}\n",
    "            dataDim=0\n",
    "            for datclass in dataClasses:\n",
    "                if self.isSequence[datclass]==False:\n",
    "                    oneHotEncoders[datclass]=self.buildEncoder(classLabels[datclass])\n",
    "                    encodingLengths[datclass]=classLabels[datclass].size\n",
    "                    #dataDim=dataDim+encodingLengths[datclass]\n",
    "                else:\n",
    "                    if self.isNominal[datclass]:\n",
    "                        raise(NotImplementedError(\"Nominal data types for sequences have not yet been implemented\"))\n",
    "                    else:\n",
    "                        encodingLengths[datclass]=1\n",
    "                        #dataDim=dataDim+1\n",
    "            \n",
    "            #Set all of the summary information to self properties\n",
    "            self.numDataPoints=numDataPoints\n",
    "            self.encodingLengths=encodingLengths#A dictionary that maps attributes to the lengths of their vector encoding schemes\n",
    "            self.oneHotEncoders=oneHotEncoders#A dictionary of dictionaries where the outer dictionary maps attributes to encoding schemes and where each encoding scheme is a dictionary that maps attribute values to one hot encodings\n",
    "            #self.dataDim=dataDim#The sum of all the encoding lengths for the relevant attributes\n",
    "            #self.isSequence=#A dictionary that returns whether an attribute takes the form of a sequence of data\n",
    "            #self.isNominal=#A dictionary that returns whether an attribute is nominal in form (neither numeric nor ordinal)\n",
    "        \n",
    "            #Save that summary file so that it can be used next time\n",
    "            self.writeSummaryFile()\n",
    "        self.MetaDataLoaded=True \n",
    "        \n",
    "    def oneHot(self, dataPoint, att):\n",
    "        #Takes the current data point and the attribute type and uses the data schema to provide the one-hot encoding for the variable\n",
    "        dataValue=dataPoint[att]       \n",
    "        #Use a stored schema dictionary to return the correct encoding scheme for the attribute (an encoding scheme is also a dictionary)\n",
    "        encoder=self.oneHotEncoders[att]\n",
    "        #Use this encoding scheme to get the encoding\n",
    "        encoding=encoder[dataValue]\n",
    "        return encoding\n",
    "    \n",
    "    def isList(self, attData):\n",
    "        #checks whether the variable attData is a list and returns true or false\n",
    "        return isinstance(attData, list)\n",
    "        #might want to try isSubclass(attData, list) if this doesn't work...\n",
    "        \n",
    "    \n",
    "    def getDataPointLength(self, dataPoint):\n",
    "        #Checks a single attribute. If the length of all sequence attributes is not equal, additional code will need to be written...\n",
    "        return len(dataPoint[\"heart_rate\"])#tries \"heart_rate\"\n",
    "\n",
    "        \n",
    "    def nextBatch(self, batch_size):\n",
    "        #Returns a tensorflow tensor (a numpy array) containing a batch of data\n",
    "        #Can be used directly for feed or to preprocess for additional efficiency\n",
    "        \n",
    "        #Currently does not explicitly separate exercise routines. \n",
    "        #Can be augmented with a variable that captures end and begnning of a routine if this helps.\n",
    "        \n",
    "        if self.dataSchemaLoaded==False:\n",
    "            raise(RuntimeError(\"Need to load a data schema\"))\n",
    "        \n",
    "        dataBatch = np.zeros((batch_size, self.dataDim))\n",
    "        #self.dataDim is the total concatenated length of the data at each time point (for all attributes)\n",
    "                \n",
    "        if self.currentDataPoint is None: #If starting an epoch, grab the first data point\n",
    "            self.currentDataPoint=self.getNextDataPoint()\n",
    "            self.currentDataPointNumber=self.currentDataPointNumber+1\n",
    "        currentDataPointLength=self.getDataPointLength(self.currentDataPoint)\n",
    "            \n",
    "        for i in range(batch_size):\n",
    "            #Need code for getting the current data point and iterating through it until the end of it...\n",
    "            #if end of data point:\n",
    "                #currentPoint = next data point\n",
    "            dataList = [] #A mutable data structure to allow us to construct the data instance...\n",
    "            if self.dataPointPosition==currentDataPointLength: #Check to se if new data point is needed\n",
    "                try:\n",
    "                    self.currentDataPoint=self.getNextDataPoint()\n",
    "                    self.currentDataPointNumber=self.currentDataPointNumber+1\n",
    "                except: #If there is no more data, return what you have\n",
    "                    return dataBatch #May need to pad this??\n",
    "                currentDataPointLength=self.getDataPointLength(self.currentDataPoint)\n",
    "                self.dataPointPosition=0\n",
    "            for j, att in enumerate(self.attributes):\n",
    "                if self.isSequence[att]: #Need to limit the sequence to the end of the batch...\n",
    "                    #Put the sequence attributes in their proper positions in the tensor array\n",
    "                    #These are numeric encoding schemes.\n",
    "                    attData=self.currentDataPoint[att][self.dataPointPosition]#Get the next entry in the attribute sequence for the current data point\n",
    "                else:\n",
    "                    #Put the context attributes in their proper positions in the tensor array\n",
    "                    #These are a one-hot encoding schemes except in the case of \"age\" and the like\n",
    "                    if self.isNominal[att]:#Checks whether the data is nominal\n",
    "                        attData = self.oneHot(self.currentDataPoint, att) #returns a list\n",
    "                    else:\n",
    "                        attData = self.currentDataPoint #Handles ordinal and numeric data\n",
    "                        \n",
    "                scaledAttData=self.scaleData(attData, att)#Rescales data if needed\n",
    "                if self.isList(scaledAttData):\n",
    "                    dataList.extend(scaledAttData)\n",
    "                else:\n",
    "                    dataList.append(scaledAttData)           \n",
    "            if len(dataList)==self.dataDim:\n",
    "                dataBatch[i,:]=dataList\n",
    "            else:\n",
    "                print(\"Data list length: \" + dataList)\n",
    "                print(\"Data schema length: \" + self.dataDim)\n",
    "                raise(ValueError(\"Data is not formatted according to the schema\"))\n",
    "                \n",
    "            self.dataPointPosition=self.dataPointPosition+1\n",
    "                                            \n",
    "        return dataBatch\n",
    "    \n",
    "    \"\"\"Should use three levels of grouping for data: the epoch, the batch, and the element, \n",
    "    where the element is equivalent to an individual exercise routine (or an individual review, etc.)\n",
    "    After each element, the state of the rnn should be reset so as to properly separate them. \n",
    "    I am not sure yet how to best initialize the RNN state...\"\"\"\n",
    "    \n",
    "    def endoIterator(self, batch_size, num_steps):\n",
    "        \"\"\"This function gets called as an iterator and returns a new batch of data where each data point is an (x,y) tuple\n",
    "        where the x is the inputs and the y is the labels.\n",
    "        A batch is returned as an numpy array (a 3-tensor) of batch_size by num_steps by self.dataDim\n",
    "        where each row is length num_steps and is shifted over by one element.\n",
    "        There are batch_size number of rows.\"\"\"\n",
    "        \n",
    "        #This should work except for data type/size problems\n",
    "\n",
    "        #raw_data = np.array(raw_data, dtype=np.int32)\n",
    "\n",
    "        data_len = self.numDataPoints\n",
    "        batch_len = data_len // batch_size\n",
    "        epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "        if epoch_size == 0:\n",
    "            raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "        #For these guys, the labels are simply the next sequence. This is to train the model to reprodue the text.\n",
    "        #Since I am not really trying to do this, I should generate the labels seperately.\n",
    "        #However, I might find that training the net this way (to predict the sequence) and then transplanting the weights into the full model might be useful...\n",
    "        \"\"\"for i in range(epoch_size):\n",
    "            batchData=self.nextBatch(batch_size)\n",
    "            data = np.zeros([batch_size, batch_len, self.dataDim])\n",
    "            for j in range(batch_size):\n",
    "                data[j] = batchData[batch_len * j:batch_len * (j + 1)]\n",
    "            x = data[:, i*num_steps:(i+1)*num_steps]\n",
    "            y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "            yield (x, y)\"\"\"\n",
    "        \n",
    "        #The code below is not ideal because it trains everything in order whereby each batch is comprised sequentially of the data\n",
    "        #It should be OK for basic testing, however.\n",
    "        #and it may miss some transitions (not sure)\n",
    "        #print( epoch_size)\n",
    "        for i in range(epoch_size):\n",
    "            batchData=self.nextBatch(batch_size*(num_steps+1))\n",
    "            #print(batchData.shape)\n",
    "            data = np.zeros([batch_size, num_steps+1, self.dataDim])\n",
    "            for j in range(batch_size):\n",
    "                data[j,:,:] = batchData[(num_steps * j):((num_steps * (j + 1))+1),:]\n",
    "            #print(data.shape)\n",
    "            #x = data[:, i*num_steps:((i+1)*num_steps),:]\n",
    "            x = data[:, 0:num_steps, :]\n",
    "            #y = data[:, i*(num_steps+1):((i+1)*num_steps+1),:]\n",
    "            y = data[:, 1:(num_steps+1), :]\n",
    "            yield (x, y)\n",
    "\n",
    "    def scaleData(self, data, att):\n",
    "        #This function provides optional rescaling of the data for optimal neural network performance. \n",
    "        #It can either be run online or offline w/ results stored in a preprocessed data file (more effecient)\n",
    "        if att==\"speed\":\n",
    "            scaledData=data\n",
    "            return scaledData\n",
    "        elif att==\"heart_rate\":\n",
    "            scaledData=data/250.0 #This will be replaced with an auto-ranging version\n",
    "            return scaledData\n",
    "        elif att==\"altitude\":\n",
    "            scaledData=float(data)/10000.0 #This will be replaced with an auto-ranging version\n",
    "            return scaledData\n",
    "        else:\n",
    "            return data\n",
    "            \n",
    "    \n",
    "    \"\"\"Might also want to define a function that returns a batch where \n",
    "    each non-sequential attribute is copied so as to be as long as the longest sequential attribute in the attribute list.\"\"\"\n",
    "        \n",
    "        \n",
    "    \"\"\"Write a function that returns data (a batch of data) in the format defined by the following placeholder tensors:\n",
    "        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        \n",
    "        Might also be ok with having a third placeholder:\n",
    "        self._context = tf.placeholder(tf.int32, [batch_size])\n",
    "        \n",
    "        Might also want to be able to return a generator that iterates (enumerates) on a batch data point by data point.\n",
    "        \n",
    "        Alternatively, I could have a generator that iterates on a batch and then another generator that iterates \n",
    "        on the individual elements of a particular data point (sequence and context)\"\"\"\n",
    "        \n",
    "    \n",
    "    def newEpoch(self):\n",
    "        #A convenience function for reseting the data loader to start a new epoch\n",
    "        self.createGenerator()\n",
    "        self.currentDataPoint=None\n",
    "        self.currentDataPointNumber=0#The number of data points looked at so far\n",
    "        self.dataPointPosition=0#The position within a data point (within an exercise)\n",
    "        \n",
    "class metaDataEndomondo(object):\n",
    "    #For disk storage of metadata\n",
    "    #Meant to be pickled and unpickled\n",
    "    def __init__(self, numDataPoints, encodingLengths, oneHotEncoders, isSequence, isNominal):\n",
    "        self.numDataPoints=numDataPoints\n",
    "        self.encodingLengths=encodingLengths\n",
    "        self.oneHotEncoders=oneHotEncoders\n",
    "        #self.dataDim=dataDim\n",
    "        self.isSequence=isSequence\n",
    "        self.isNominal=isNominal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
