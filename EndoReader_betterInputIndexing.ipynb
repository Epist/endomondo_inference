{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import ijson\n",
    "import pickle\n",
    "import os\n",
    "import jsonReader\n",
    "from haversine import haversine\n",
    "\n",
    "#Need to:\n",
    "        #0. Scan the file when producing metadata and create a list of data point indices\n",
    "        #1. Load the list of data point indices\n",
    "        #2. Randomly permute the list of data point indices\n",
    "        #3. Split the list of data point indices into training, validation, and test sets\n",
    "        #4. Provide a method to get the next data point in the list (and either globally save the position of the list or save it implicitly in a generator)\n",
    "        #5. Provide a method to reset this list position (for a new epoch)\n",
    "        #6. Rewrite the endoIterator method to respect randomization (copy the random access iterator code from the ptb reader example)\n",
    "\n",
    "class dataInterpreter(object):\n",
    "    def __init__(self, fn=\"endomondoHR_proper.json\", attributes=None, allowMissingData=True, scaleVals=True):\n",
    "        self.dataFileName=fn#Will eventually replace this with a data folder name\n",
    "        self.dataFile=open(self.dataFileName, 'r')\n",
    "        self.MetaDataLoaded=False\n",
    "        self.dataSchemaLoaded=False\n",
    "        #self.currentDataPoint=None\n",
    "        self.dataPointPosition=0\n",
    "        self.attIgnore=['id','url','speed']#Attributes to ignore when building metadata\n",
    "        self.metaDataFn=fn[0:len(fn)-5]+\"_metaData.p\"\n",
    "        self.allowMissingData=allowMissingData\n",
    "        self.scaleVals=scaleVals\n",
    "        #self.valTestSplit=(.1,.1)\n",
    "        if attributes is not None:\n",
    "            self.buildDataSchema(attributes)\n",
    "    \n",
    "    def buildDataSchema(self, attributes, targetAtt, trainValTestSplit=(.8,.1,.1)):\n",
    "        self.targetAtt=targetAtt\n",
    "        self.buildMetaData()\n",
    "        self.splitForValidation(trainValTestSplit)\n",
    "        #self.newEpoch()#Reset all indices and counters\n",
    "        self.attributes=attributes\n",
    "        dataDimSum=0\n",
    "        for att in self.attributes:\n",
    "            dataDimSum=dataDimSum+self.encodingLengths[att]\n",
    "        self.dataDim=dataDimSum\n",
    "        \n",
    "        #Create a dictionary that takes an attribute and returns a beginning and end position of the attribute in the input sequence\n",
    "        self.inputAttributes =[x for x in self.attributes if x != targetAtt]\n",
    "        self.inputIndices={}\n",
    "        lastIndex=0\n",
    "        for att in self.inputAttributes:\n",
    "            nextIndex=lastIndex+self.encodingLengths[att]\n",
    "            self.inputIndices[att]=(lastIndex, nextIndex)\n",
    "            lastIndex=nextIndex\n",
    "            \n",
    "        self.dataSchemaLoaded=True\n",
    "    \n",
    "    def createSequentialGenerator(self): #Define a new data generator\n",
    "        filename = self.dataFileName\n",
    "        self.f=open(filename, 'r')\n",
    "        objects = ijson.items(self.f, 'users.item')\n",
    "        self.dataObjects=objects\n",
    "        return self.dataObjects\n",
    "    \n",
    "    def dataGenerator(self, dataSetOrder):\n",
    "        for dp_index in dataSetOrder:\n",
    "            fileIndices = self.dataPointIndices[dp_index]\n",
    "            potentialNextDataPoint=jsonReader.getDataPoint(fileIndices, self.dataFile)\n",
    "            \n",
    "            \"\"\"\n",
    "            if self.allowMissingData==False:\n",
    "                #Check if the next data point contains all the requested attributes\n",
    "                for i, att in enumerate(self.attributes):\n",
    "                    #print(att)\n",
    "                    try:\n",
    "                        test=self.currentDataPoint[att]\n",
    "                    except:\n",
    "                        print(\"Skipping data point because it lacks attribute: \" + att)\n",
    "                        #print(\"Skipping data point because it lacks attribute\")\n",
    "                        return self.getNextDataPoint() #Try the next one instead\n",
    "            \"\"\"\n",
    "            yield potentialNextDataPoint #returns next data point\n",
    "    \n",
    "    def randomizeDataOrder(self, dataIndices):\n",
    "        return np.random.permutation(dataIndices)\n",
    "\n",
    "    #def getNextDataPoint(self):\n",
    "    #        jsonReader.getDataPoint(index, dataFile)\n",
    "    #    return dataPoint\n",
    "\n",
    "    def getNextDataPointSequential(self):\n",
    "        try: #If there is a generator already defined\n",
    "            objects=self.dataObjects\n",
    "        except: #Otherwise create a new one\n",
    "            #Creating new generator\n",
    "            objects=self.createSequentialGenerator()\n",
    "        nextDataPoint=self.__convert(objects.next())\n",
    "\n",
    "        return nextDataPoint\n",
    "    \n",
    "    #def newEpoch(self):\n",
    "        # A convenience function for reseting the data loader to start a new epoch\n",
    "        #self.currentDataPoint = None\n",
    "    #    self.dataPointPosition = 0  # The position within a data point (within an exercise)       \n",
    "    \n",
    "    \n",
    "    def batchIteratorSupervised(self, batch_size, trainValidTest, targetAtt):\n",
    "        #Performs the same job as the batch iterator, but with one of the attributes separated as the supervision signal\n",
    "        inputAttributes = self.inputAttributes\n",
    "        inputDataDim=self.getInputDim(targetAtt)\n",
    "        targetDataDim=self.getTargetDim(targetAtt)\n",
    "\n",
    "        if trainValidTest == 'train':\n",
    "            self.trainingOrder = self.randomizeDataOrder(self.trainingSet)\n",
    "            dataGen = self.dataGenerator(self.trainingOrder)\n",
    "        elif trainValidTest == 'valid':\n",
    "            self.validationOrder = self.randomizeDataOrder(self.validationSet)\n",
    "            dataGen = self.dataGenerator(self.validationOrder)\n",
    "        elif trainValidTest == 'test':\n",
    "            self.testOrder = self.randomizeDataOrder(self.testSet)\n",
    "            dataGen = self.dataGenerator(self.testOrder)\n",
    "        else:\n",
    "            raise (Exception(\"Invalid dataset type. Must be 'train', 'valid', or 'test'\"))\n",
    "\n",
    "        if self.dataSchemaLoaded == False:\n",
    "            raise (RuntimeError(\"Need to load a data schema\"))\n",
    "\n",
    "        inputDataBatch = np.zeros((batch_size, inputDataDim))\n",
    "        # inputDataDim is the total concatenated length of the data at each time point (for all input attributes)\n",
    "        targetDataBatch = np.zeros((batch_size, targetDataDim))\n",
    "\n",
    "\n",
    "        # if currentDataPoint is None: #If starting an epoch, grab the first data point\n",
    "        currentDataPoint = dataGen.next()\n",
    "        dataPointPosition = 0\n",
    "        currentDataPointLength = self.getDataPointLength(currentDataPoint)\n",
    "        moreData = True\n",
    "        currentDerivedData={}\n",
    "        while moreData:\n",
    "            for i in range(batch_size):\n",
    "                # Need code for getting the current data point and iterating through it until the end of it...\n",
    "                # if end of data point:\n",
    "                # currentPoint = next data point\n",
    "                dataList = []  # A mutable data structure to allow us to construct the data instance...\n",
    "                if dataPointPosition == currentDataPointLength:  # Check to see if new data point is needed\n",
    "                    try:\n",
    "                        currentDataPoint = dataGen.next()\n",
    "                    except:  # If there is no more data, return what you have\n",
    "                        moreData = False\n",
    "                        yield [inputDataBatch, targetDataBatch]  # May need to pad this??\n",
    "                    currentDataPointLength = self.getDataPointLength(currentDataPoint)\n",
    "                    currentDerivedData = {} #Reset the derived data dictionary\n",
    "                    dataPointPosition = 0\n",
    "                for j, att in enumerate(inputAttributes):\n",
    "                    if self.isDerived[att]:\n",
    "                        #handle the derived variables\n",
    "                        if att in currentDerivedData.keys():\n",
    "                            #Use the data from the current data point position\n",
    "                            attDataPoint=currentDerivedData[att]\n",
    "                            attData = attDataPoint[dataPointPosition]\n",
    "                        else:\n",
    "                            #Generate the data and then use the data from the current data point position which should be 0\n",
    "                            currentDerivedData[att] = self.deriveData(att, currentDataPoint)\n",
    "                            attDataPoint=currentDerivedData[att]\n",
    "                            attData = attDataPoint[dataPointPosition]\n",
    "                    else:\n",
    "                        if self.isSequence[att]:  # Need to limit the sequence to the end of the batch...\n",
    "                            # Put the sequence attributes in their proper positions in the tensor array\n",
    "                            # These are numeric encoding schemes.\n",
    "                            attData = currentDataPoint[att][\n",
    "                                dataPointPosition]  # Get the next entry in the attribute sequence for the current data point\n",
    "                        else:\n",
    "                            # Put the context attributes in their proper positions in the tensor array\n",
    "                            # These are a one-hot encoding schemes except in the case of \"age\" and the like\n",
    "                            if self.isNominal[att]:  # Checks whether the data is nominal\n",
    "                                attData = self.oneHot(currentDataPoint, att)  # returns a list\n",
    "                            else:\n",
    "                                attData = currentDataPoint  # Handles ordinal and numeric data\n",
    "\n",
    "                    scaledAttData = self.scaleData(attData, att)  # Rescales data if needed\n",
    "                    if self.isList(scaledAttData):\n",
    "                        dataList.extend(scaledAttData)\n",
    "                    else:\n",
    "                        dataList.append(scaledAttData)\n",
    "                #Now do the same for the target attribute\n",
    "                if self.isSequence[targetAtt]:  # Need to limit the sequence to the end of the batch...\n",
    "                    # Put the target attribute in its proper positions in the tensor array\n",
    "                    # These are numeric encoding schemes.\n",
    "                    attData = currentDataPoint[targetAtt][\n",
    "                        dataPointPosition]  # Get the next entry in the attribute sequence for the current data point\n",
    "                else:\n",
    "                    # Put the context attributes in their proper positions in the tensor array\n",
    "                    # These are a one-hot encoding schemes except in the case of \"age\" and the like\n",
    "                    if self.isNominal[targetAtt]:  # Checks whether the data is nominal\n",
    "                        attData = self.oneHot(currentDataPoint, targetAtt)  # returns a list\n",
    "                    else:\n",
    "                        attData = currentDataPoint  # Handles ordinal and numeric data\n",
    "\n",
    "                scaledTargetAttData = self.scaleData(attData, targetAtt)  # Rescales data if needed\n",
    "                targetDataBatch[i,:] = scaledTargetAttData#Add the target data for the current data point to the full list\n",
    "\n",
    "                #Check length of input data vector\n",
    "                if len(dataList) == inputDataDim:\n",
    "                    inputDataBatch[i, :] = dataList\n",
    "                else:\n",
    "                    print(\"Data list length: \" + str(len(dataList)))\n",
    "                    print(\"Data schema length: \" + str(len(inputDataDim)))\n",
    "                    raise (ValueError(\"Data is not formatted according to the schema\"))\n",
    "\n",
    "                dataPointPosition = dataPointPosition + 1\n",
    "\n",
    "            yield [inputDataBatch, targetDataBatch]\n",
    "            \n",
    "    def dataIteratorSupervised(self, trainValidTest):\n",
    "        targetAtt=self.targetAtt\n",
    "        #Performs the same job as the batch iterator, but with one of the attributes separated as the supervision signal\n",
    "        inputAttributes = self.inputAttributes\n",
    "        inputDataDim=self.getInputDim(targetAtt)\n",
    "        targetDataDim=self.getTargetDim(targetAtt)\n",
    "\n",
    "\n",
    "        if trainValidTest == 'train':\n",
    "            self.trainingOrder = self.randomizeDataOrder(self.trainingSet)\n",
    "            dataGen = self.dataGenerator(self.trainingOrder)\n",
    "        elif trainValidTest == 'valid':\n",
    "            self.validationOrder = self.randomizeDataOrder(self.validationSet)\n",
    "            dataGen = self.dataGenerator(self.validationOrder)\n",
    "        elif trainValidTest == 'test':\n",
    "            self.testOrder = self.randomizeDataOrder(self.testSet)\n",
    "            dataGen = self.dataGenerator(self.testOrder)\n",
    "        else:\n",
    "            raise (Exception(\"Invalid dataset type. Must be 'train', 'valid', or 'test'\"))\n",
    "\n",
    "        if self.dataSchemaLoaded == False:\n",
    "            raise (RuntimeError(\"Need to load a data schema\"))\n",
    "\n",
    "        #inputData = np.zeros((batch_size, inputDataDim))\n",
    "        # inputDataDim is the total concatenated length of the data at each time point (for all input attributes)\n",
    "        #targetData = np.zeros((batch_size, targetDataDim))\n",
    "\n",
    "\n",
    "        # if currentDataPoint is None: #If starting an epoch, grab the first data point\n",
    "        currentDataPoint = dataGen.next()\n",
    "        dataPointPosition = 0\n",
    "        currentDataPointLength = self.getDataPointLength(currentDataPoint)\n",
    "        moreData = True\n",
    "        currentDerivedData={}\n",
    "        while moreData:\n",
    "            # Need code for getting the current data point and iterating through it until the end of it...\n",
    "            # if end of data point:\n",
    "            # currentPoint = next data point\n",
    "            dataList = []  # A mutable data structure to allow us to construct the data instance...\n",
    "            if dataPointPosition == currentDataPointLength:  # Check to see if new data point is needed\n",
    "                try:\n",
    "                    currentDataPoint = dataGen.next()\n",
    "                except:  # If there is no more data, return what you have\n",
    "                    moreData = False\n",
    "                    yield [inputData, targetData]  # May need to pad this??\n",
    "                currentDataPointLength = self.getDataPointLength(currentDataPoint)\n",
    "                currentDerivedData = {} #Reset the derived data dictionary\n",
    "                dataPointPosition = 0\n",
    "            for j, att in enumerate(inputAttributes):\n",
    "                if self.isDerived[att]:\n",
    "                    #handle the derived variables\n",
    "                    if att in currentDerivedData.keys():\n",
    "                        #Use the data from the current data point position\n",
    "                        attDataPoint=currentDerivedData[att]\n",
    "                        attData = attDataPoint[dataPointPosition]\n",
    "                    else:\n",
    "                        #Generate the data and then use the data from the current data point position which should be 0\n",
    "                        currentDerivedData[att] = self.deriveData(att, currentDataPoint)\n",
    "                        attDataPoint=currentDerivedData[att]\n",
    "                        attData = attDataPoint[dataPointPosition]\n",
    "                else:\n",
    "                    if self.isSequence[att]:  # Need to limit the sequence to the end of the batch...\n",
    "                        # Put the sequence attributes in their proper positions in the tensor array\n",
    "                        # These are numeric encoding schemes.\n",
    "                        attData = currentDataPoint[att][\n",
    "                            dataPointPosition]  # Get the next entry in the attribute sequence for the current data point\n",
    "                    else:\n",
    "                        # Put the context attributes in their proper positions in the tensor array\n",
    "                        # These are a one-hot encoding schemes except in the case of \"age\" and the like\n",
    "                        if self.isNominal[att]:  # Checks whether the data is nominal\n",
    "                            attData = self.oneHot(currentDataPoint, att)  # returns a list\n",
    "                        else:\n",
    "                            attData = currentDataPoint  # Handles ordinal and numeric data\n",
    "\n",
    "                scaledAttData = self.scaleData(attData, att)  # Rescales data if needed\n",
    "                if self.isList(scaledAttData):\n",
    "                    dataList.extend(scaledAttData)\n",
    "                else:\n",
    "                    dataList.append(scaledAttData)\n",
    "            #Now do the same for the target attribute\n",
    "            if self.isSequence[targetAtt]:  # Need to limit the sequence to the end of the batch...\n",
    "                # Put the target attribute in its proper positions in the tensor array\n",
    "                # These are numeric encoding schemes.\n",
    "                attTarData = currentDataPoint[targetAtt][\n",
    "                    dataPointPosition]  # Get the next entry in the attribute sequence for the current data point\n",
    "            else:\n",
    "                # Put the context attributes in their proper positions in the tensor array\n",
    "                # These are a one-hot encoding schemes except in the case of \"age\" and the like\n",
    "                if self.isNominal[targetAtt]:  # Checks whether the data is nominal\n",
    "                    attTarData = self.oneHot(currentDataPoint, targetAtt)  # returns a list\n",
    "                else:\n",
    "                    attTarData = currentDataPoint  # Handles ordinal and numeric data\n",
    "\n",
    "            scaledTargetAttData = self.scaleData(attTarData, targetAtt)  # Rescales data if needed\n",
    "            targetData = [] #So that we can return it in the same list format as the inputs\n",
    "            targetData.append(scaledTargetAttData)#Add the target data for the current data point to the full list\n",
    "\n",
    "            #Check length of input data vector\n",
    "            if len(dataList) == inputDataDim:\n",
    "                inputData = dataList\n",
    "            else:\n",
    "                print(\"Data list length: \" + str(len(dataList)))\n",
    "                print(\"Data schema length: \" + str(len(inputDataDim)))\n",
    "                raise (ValueError(\"Data is not formatted according to the schema\"))\n",
    "\n",
    "            dataPointPosition = dataPointPosition + 1\n",
    "\n",
    "            yield [inputData, targetData]\n",
    "\n",
    "\n",
    "    def batchIterator(self, batch_size, trainValidTest):\n",
    "        #Returns a tensorflow tensor (a numpy array) containing a batch of data\n",
    "        #Can be used directly for feed or to preprocess for additional efficiency\n",
    "        \n",
    "        #Currently does not explicitly separate exercise routines. \n",
    "        #Can be augmented with a variable that captures end and begnning of a routine if this helps.\n",
    "        \n",
    "        if trainValidTest=='train':\n",
    "            self.trainingOrder = self.randomizeDataOrder(self.trainingSet)\n",
    "            dataGen=self.dataGenerator(self.trainingOrder)\n",
    "        elif trainValidTest=='valid':\n",
    "            self.validationOrder = self.randomizeDataOrder(self.validationSet)\n",
    "            dataGen=self.dataGenerator(self.validationOrder)\n",
    "        elif trainValidTest=='test':\n",
    "            self.testOrder = self.randomizeDataOrder(self.testSet)\n",
    "            dataGen=self.dataGenerator(self.testOrder)\n",
    "        else:\n",
    "            raise(Exception(\"Invalid dataset type. Must be 'train', 'valid', or 'test'\"))\n",
    "        \n",
    "        if self.dataSchemaLoaded==False:\n",
    "            raise(RuntimeError(\"Need to load a data schema\"))\n",
    "        \n",
    "        dataBatch = np.zeros((batch_size, self.dataDim))\n",
    "        #self.dataDim is the total concatenated length of the data at each time point (for all attributes)\n",
    "                \n",
    "        #if currentDataPoint is None: #If starting an epoch, grab the first data point\n",
    "        currentDataPoint=dataGen.next()\n",
    "        dataPointPosition=0\n",
    "        currentDataPointLength=self.getDataPointLength(currentDataPoint)\n",
    "        moreData=True\n",
    "        while moreData:\n",
    "            for i in range(batch_size):\n",
    "                #Need code for getting the current data point and iterating through it until the end of it...\n",
    "                #if end of data point:\n",
    "                    #currentPoint = next data point\n",
    "                dataList = [] #A mutable data structure to allow us to construct the data instance...\n",
    "                if dataPointPosition==currentDataPointLength: #Check to see if new data point is needed\n",
    "                    try:\n",
    "                        currentDataPoint=dataGen.next()\n",
    "                    except: #If there is no more data, return what you have\n",
    "                        moreData=False\n",
    "                        yield dataBatch #May need to pad this??\n",
    "                    currentDataPointLength=self.getDataPointLength(currentDataPoint)\n",
    "                    dataPointPosition=0\n",
    "                for j, att in enumerate(self.attributes):\n",
    "                    if self.isSequence[att]: #Need to limit the sequence to the end of the batch...\n",
    "                        #Put the sequence attributes in their proper positions in the tensor array\n",
    "                        #These are numeric encoding schemes.\n",
    "                        attData=currentDataPoint[att][dataPointPosition]#Get the next entry in the attribute sequence for the current data point\n",
    "                    else:\n",
    "                        #Put the context attributes in their proper positions in the tensor array\n",
    "                        #These are a one-hot encoding schemes except in the case of \"age\" and the like\n",
    "                        if self.isNominal[att]:#Checks whether the data is nominal\n",
    "                            attData = self.oneHot(currentDataPoint, att) #returns a list\n",
    "                        else:\n",
    "                            attData = currentDataPoint #Handles ordinal and numeric data\n",
    "\n",
    "                    scaledAttData=self.scaleData(attData, att)#Rescales data if needed\n",
    "                    if self.isList(scaledAttData):\n",
    "                        dataList.extend(scaledAttData)\n",
    "                    else:\n",
    "                        dataList.append(scaledAttData)           \n",
    "                if len(dataList)==self.dataDim:\n",
    "                    dataBatch[i,:]=dataList\n",
    "                else:\n",
    "                    print(\"Data list length: \" + dataList)\n",
    "                    print(\"Data schema length: \" + self.dataDim)\n",
    "                    raise(ValueError(\"Data is not formatted according to the schema\"))\n",
    "\n",
    "                dataPointPosition=dataPointPosition+1\n",
    "\n",
    "            yield dataBatch\n",
    "\n",
    "    def endoIteratorSupervised(self, batch_size, num_steps, trainValidTest):\n",
    "            #Does the same thing as the endoIterator, except for a model with separate supervised targets (targets are not the next element in the sequence)\n",
    "\n",
    "            batchGen = self.dataIteratorSupervised(trainValidTest)\n",
    "\n",
    "            data_len = self.numDataPoints\n",
    "            batch_len = data_len // batch_size\n",
    "            epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "            inputDataDim = self.getInputDim(self.targetAtt)\n",
    "            targetDataDim = self.getTargetDim(self.targetAtt)\n",
    "\n",
    "            if epoch_size == 0:\n",
    "                raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "                \n",
    "\n",
    "            nextRow=[next(batchGen) for x in range(num_steps)]#Fill the first row (with both inputs and targets)\n",
    "            #print(nextRowTargets)\n",
    "            for i in range(epoch_size):\n",
    "                inputData = np.zeros([batch_size*num_steps, num_steps, inputDataDim])\n",
    "                targetData = np.zeros([batch_size*num_steps, num_steps, targetDataDim])\n",
    "                for j in range(batch_size):\n",
    "                    for k in range(num_steps):\n",
    "                        #print(\" j: \" + str(j) + \" k: \" + str(k))\n",
    "                        currentRow=nextRow\n",
    "\n",
    "                        inputData[((j*num_steps)+k), :, :]  = [currentRow[x][0] for x in range(num_steps)]\n",
    "                        targetData[((j*num_steps)+k), :, :] = [currentRow[x][1] for x in range(num_steps)]\n",
    "\n",
    "                        nextRow[0:(num_steps-1)] = currentRow[1:num_steps]\n",
    "                        nextRow[num_steps-1]=batchGen.next()\n",
    "\n",
    "                x = inputData\n",
    "                y = targetData\n",
    "                yield (x, y)\n",
    "\n",
    "    \n",
    "    def endoIterator(self, batch_size, num_steps, trainValidTest):\n",
    "        #Returns a batch-wise generator for the endomondo data with inputs as the current element in the sequence and targets as the next element in the sequence\n",
    "\n",
    "        batchGen = self.batchIterator(batch_size*(num_steps+1), trainValidTest)\n",
    "\n",
    "        data_len = self.numDataPoints\n",
    "        batch_len = data_len // batch_size\n",
    "        epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "        if epoch_size == 0:\n",
    "            raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "        #For these guys, the labels are simply the next sequence. This is to train the model to reprodue the text.\n",
    "        #Since I am not really trying to do this, I should generate the labels seperately.\n",
    "        #However, I might find that training the net this way (to predict the sequence) and then transplanting the weights into the full model might be useful...\n",
    "        \"\"\"for i in range(epoch_size):\n",
    "            batchData=self.nextBatch(batch_size)\n",
    "            data = np.zeros([batch_size, batch_len, self.dataDim])\n",
    "            for j in range(batch_size):\n",
    "                data[j] = batchData[batch_len * j:batch_len * (j + 1)]\n",
    "            x = data[:, i*num_steps:(i+1)*num_steps]\n",
    "            y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "            yield (x, y)\"\"\"\n",
    "        \n",
    "        #The code below is not ideal because it trains everything in order whereby each batch is comprised sequentially of the data\n",
    "        #It should be OK for basic testing, however.\n",
    "        #and it may miss some transitions (not sure)\n",
    "        #print( epoch_size)\n",
    "        for i in range(epoch_size):\n",
    "            batchData=batchGen.next()\n",
    "            #print(batchData.shape)\n",
    "            data = np.zeros([batch_size, num_steps+1, self.dataDim])\n",
    "            for j in range(batch_size):\n",
    "                data[j,:,:] = batchData[(num_steps * j):((num_steps * (j + 1))+1),:]\n",
    "            #print(data.shape)\n",
    "            #x = data[:, i*num_steps:((i+1)*num_steps),:]\n",
    "            x = data[:, 0:num_steps, :]\n",
    "            #y = data[:, i*(num_steps+1):((i+1)*num_steps+1),:]\n",
    "            y = data[:, 1:(num_steps+1), :]\n",
    "            yield (x, y)\n",
    "        \n",
    "    \n",
    "    def splitForValidation(self, valTestSplit):\n",
    "        #Construct seperate data files for the training, test, and validation data\n",
    "        self.numDataPoints\n",
    "        trainingSetSize=int(round(self.numDataPoints*valTestSplit[0]))\n",
    "        validationSetSize=int(round(self.numDataPoints*valTestSplit[1]))\n",
    "        testSetSize=int(round(self.numDataPoints*valTestSplit[2]))\n",
    "        randomOrder=self.randomizeDataOrder(self.numDataPoints)\n",
    "        \n",
    "        self.trainingSet=randomOrder[0:trainingSetSize]\n",
    "        self.validationSet=randomOrder[trainingSetSize:trainingSetSize+validationSetSize]\n",
    "        self.testSet=randomOrder[trainingSetSize+validationSetSize:trainingSetSize+validationSetSize+testSetSize]\n",
    "        \n",
    "        #print(\"training set size:\" + str(len(self.trainingSet))\n",
    "        #print(\"validation set size:\" + str(len(self.validationSet))\n",
    "        #print(\"test set size:\" + str(len(self.testSet)))\n",
    "    \n",
    "    def decoderKey(self):\n",
    "        return [x for x in self.attributes if x != targetAtt]\n",
    "    \n",
    "    def dataDecoder(self, dataPoints):\n",
    "        convertedData=[]\n",
    "        for dp in dataPoints:\n",
    "            convertedData.append(self.dataDecoderDP(dp))\n",
    "        return convertedData\n",
    "    \n",
    "    def dataDecoderDP(self, dataPoint):\n",
    "        #This function takes an encoded data point (a single time step) and decodes it into a readable set of variables \n",
    "        #for use in statistical processing and visualization\n",
    "        inputAttributes = [x for x in self.attributes if x != self.targetAtt]\n",
    "        #inputDataDim=self.getInputDim(inputAttributes)\n",
    "        \n",
    "        try:\n",
    "            inverseEncoders=self.inverseOneHotEncoders\n",
    "        except:\n",
    "            self.inverseOneHotEncoders=self.invertOneHots()\n",
    "            inverseEncoders=self.inverseOneHotEncoders\n",
    "        \n",
    "        decodedDataPoint=[]\n",
    "        dataPointPosition=0\n",
    "        for i, att in enumerate(inputAttributes):\n",
    "            attLength = self.encodingLengths[att]\n",
    "            currentAttData = dataPoint[dataPointPosition:dataPointPosition+attLength]\n",
    "            dataPointPosition = dataPointPosition + attLength\n",
    "            if self.isSequence[att]==False:\n",
    "                currentEncoder=inverseEncoders[att]\n",
    "                decodedDataPoint.append(currentEncoder[str([int(i) for i in list(currentAttData)])])\n",
    "            else:\n",
    "                decodedDataPoint.append(currentAttData[0])\n",
    "\n",
    "        return decodedDataPoint\n",
    "\n",
    "    def deriveData(self, att, currentDataPoint):\n",
    "        if att=='time_elapsed':\n",
    "            #Derive the time elapsed data sequence\n",
    "            timestamps=currentDataPoint['timestamp']\n",
    "            initialTime=timestamps[0]\n",
    "            return [x-initialTime for x in timestamps]\n",
    "            #Going to need to scale these appropriately in the scaling function\n",
    "        elif att=='distance':\n",
    "            #Derive the distance data sequence\n",
    "            lats=currentDataPoint['latitude']\n",
    "            longs=currentDataPoint['longitude']\n",
    "            indices=range(1, len(lats)) #Creates a list of indices from 1 to the length of the seuqnces to index all elements but the first (element zero)\n",
    "            distances=[0]\n",
    "            distances.extend([haversine([lats[i-1],longs[i-1]], [lats[i],longs[i]]) for i in indices]) #Gets distance traveled since last time point in kilometers\n",
    "            return distances\n",
    "        elif att=='new_workout':\n",
    "            workoutLength=len(currentDataPoint['timestamp'])#This is likely always equal to 500\n",
    "            newWorkout=np.zeros(workoutLength)\n",
    "            newWorkout[0]=1 #Add the signal\n",
    "            return newWorkout\n",
    "        elif att=='derived_speed':\n",
    "            distances=self.deriveData('distance', currentDataPoint)\n",
    "            timestamps=currentDataPoint['timestamp']\n",
    "            indices=range(1, len(timestamps))\n",
    "            times=[0]\n",
    "            times.extend([timestamps[i] - timestamps[i-1] for i in indices])\n",
    "            derivedSpeeds=[0]\n",
    "            try:\n",
    "                derivedSpeeds.extend([distances[i]/times[i] for i in indices])\n",
    "            except:\n",
    "                #print(\"Exception in deriving speeds\")\n",
    "                for i in indices:\n",
    "                    try:\n",
    "                        derivedSpeeds.append(distances[i]/times[i])\n",
    "                    except:\n",
    "                        derivedSpeeds.append(0)\n",
    "            return derivedSpeeds\n",
    "        else:\n",
    "            raise(Exception(\"No such derived data attribute\"))\n",
    "            \n",
    "    \"\"\"def scaleData(self, data, att):\n",
    "        #This function provides optional rescaling of the data for optimal neural network performance. \n",
    "        #It can either be run online or offline w/ results stored in a preprocessed data file (more effecient)\n",
    "        if self.scaleVals:\n",
    "            if att==\"speed\":\n",
    "                scaledData=data\n",
    "                return scaledData\n",
    "            elif att==\"heart_rate\":\n",
    "                scaledData=data/250.0 #This will be replaced with an auto-ranging version\n",
    "                return scaledData\n",
    "            elif att==\"altitude\":\n",
    "                scaledData=float(data)/10000.0 #This will be replaced with an auto-ranging version\n",
    "                return scaledData\n",
    "            else:\n",
    "                return data\n",
    "        else:\n",
    "            return data\"\"\"\n",
    "        \n",
    "    def scaleData(self, data, att, zMultiple=2):\n",
    "        #This function scales the data based on precomputed means and standard deviations\n",
    "        #It does this by computing z-scores and multiplying them based on a scaling paramater\n",
    "        #It therefore produces zero-centered data, which is important for the drop-in procedure\n",
    "        if self.scaleVals:\n",
    "            zScore = (data-self.variableMeans[att])/self.variableStds[att]\n",
    "            return zScore * zMultiple\n",
    "        else:\n",
    "            return data\n",
    "        \n",
    "    def __convert(self, unicData): #Converts the unicode text in a dictionary to ascii\n",
    "        #Shamelessly lifted from http://stackoverflow.com/questions/13101653/python-convert-complex-dictionary-of-strings-from-unicode-to-ascii\n",
    "        if isinstance(unicData, dict):\n",
    "            return {self.__convert(key): self.__convert(value) for key, value in unicData.iteritems()}\n",
    "        elif isinstance(unicData, list):\n",
    "            return [self.__convert(element) for element in unicData]\n",
    "        elif isinstance(unicData, unicode):\n",
    "            return unicData.encode('utf-8')\n",
    "        else:\n",
    "            return unicData\n",
    "        \n",
    "    def getDataPointLength(self, dataPoint):\n",
    "        #Checks a single attribute. If the length of all sequence attributes is not equal, additional code will need to be written...\n",
    "        return len(dataPoint[\"heart_rate\"])#tries \"heart_rate\"\n",
    "    \n",
    "    def isList(self, attData):\n",
    "        #checks whether the variable attData is a list and returns true or false\n",
    "        return isinstance(attData, list)\n",
    "        #might want to try isSubclass(attData, list) if this doesn't work...\n",
    "    \n",
    "    def buildEncoder(self, classLabels):\n",
    "        #Constructs a dictionary that maps each class label to a list (encoding scheme) where one entry in the list is 1 and the remainder are 0\n",
    "        encodingLength=classLabels.size\n",
    "        encoder={}\n",
    "        for i, label in enumerate(classLabels):\n",
    "            encoding=[0] * encodingLength\n",
    "            encoding[i]=1\n",
    "            encoder[label]=encoding\n",
    "        return encoder\n",
    "    \n",
    "    def getDataLabels(self, data, dataClass):\n",
    "        #The \"data\" argument is in the same format as is returned by \"getNdatapoints\"\n",
    "        #If there is a use case that involves finding all the possible labels for a given class, a seperate function should be written to save memory usage...\n",
    "        class_labels = [col[dataClass] for col in data]\n",
    "        return np.unique(np.array(class_labels))\n",
    "    \n",
    "    def writeSummaryFile(self):\n",
    "        metaDataForWriting=metaDataEndomondo(self.numDataPoints, self.encodingLengths, self.oneHotEncoders, self.isSequence, \n",
    "                                             self.isNominal, self.isDerived, self.dataPointIndices, self.variableMeans, self.variableStds)\n",
    "        with open(self.metaDataFn, \"wb\") as f:\n",
    "            pickle.dump(metaDataForWriting, f)\n",
    "\n",
    "        #pickle.dump(metaDataForWriting, open(self.metaDataFn, \"wb\"))\n",
    "        print(\"Summary file written\")\n",
    "        \n",
    "    def loadSummaryFile(self):\n",
    "        try:\n",
    "            print(\"Loading metadata\")\n",
    "            with open(self.metaDataFn, \"rb\") as f:\n",
    "                metaData = pickle.load(f)\n",
    "                \n",
    "            #metaData=pickle.load(open(self.metaDataFn, \"rb\"))\n",
    "        except:\n",
    "            raise(IOError(\"Metadata file: \" + self.metaDataFn + \" not in valid pickle format\"))\n",
    "        self.numDataPoints=metaData.numDataPoints\n",
    "        self.encodingLengths=metaData.encodingLengths\n",
    "        self.oneHotEncoders=metaData.oneHotEncoders\n",
    "        #self.dataDim=metaData.dataDim\n",
    "        self.isSequence=metaData.isSequence\n",
    "        self.isNominal=metaData.isNominal\n",
    "        self.isDerived=metaData.isDerived\n",
    "        self.dataPointIndices=metaData.dataPointIndices\n",
    "        self.variableMeans = metaData.variableMeans\n",
    "        self.variableStds = metaData.variableStds\n",
    "        print(\"Metadata loaded\")\n",
    "        \n",
    "    def buildMetaData(self):\n",
    "        #Takes a list of attributes and the current datafile and constructs a schema for the data to be input into the RNN.\n",
    "        if os.path.isfile(self.metaDataFn):#If a summary file exists\n",
    "            self.loadSummaryFile()#Load that summary file and use it to capture all the necessary info\n",
    "        else:\n",
    "            print(\"Building data schema\")\n",
    "            #Build such a summary file by running through the full dataset and capturing the necessary statistics\n",
    "            self.isSequence={'altitude':True, 'gender':False, 'heart_rate':True, 'id':False, 'latitude':True, 'longitude':True,\n",
    "                             'speed':True, 'sport':False, 'timestamp':True, 'url':False, 'userId':False, 'time_elapsed': True, \n",
    "                             'distance':True, 'new_workout':True, 'derived_speed':True}#Handcoded\n",
    "            self.isNominal={'altitude':False, 'gender':True, 'heart_rate':False, 'id':True, 'latitude':False, 'longitude':False,\n",
    "                            'speed':False, 'sport':True, 'timestamp':False, 'url':True, 'userId':True, 'time_elapsed': False,\n",
    "                            'distance':False, 'new_workout':False, 'derived_speed':False}#Handcoded\n",
    "            self.isDerived={'altitude':False, 'gender':False, 'heart_rate':False, 'id':False, 'latitude':False, 'longitude':False,\n",
    "                   'speed':False, 'sport':False, 'timestamp':False, 'url':False, 'userId':False, 'time_elapsed': True,\n",
    "                            'distance':True, 'new_workout':True, 'derived_speed':True}#Handcoded\n",
    "            allAttributes=['altitude', 'gender', 'heart_rate', 'id', 'latitude', 'longitude',\n",
    "                   'speed', 'sport', 'timestamp', 'url', 'userId', 'time_elapsed', 'distance', 'new_workout', 'derived_speed']\n",
    "            attributes=[x for x in allAttributes if x not in self.attIgnore]#get rid of the attributes that we are ignoring\n",
    "            \n",
    "            variableSums={'altitude':0, 'gender':0, 'heart_rate':0, 'id':0, 'latitude':0, 'longitude':0,\n",
    "                   'speed':0, 'sport':0, 'timestamp':0, 'url':0, 'userId':0, 'time_elapsed': 0,\n",
    "                            'distance':0, 'new_workout':0, 'derived_speed':0}\n",
    "            \n",
    "            #self.newEpoch()#makes sure to reset things\n",
    "            moreData=True\n",
    "            classLabels={}\n",
    "            numDataPoints=0\n",
    "            while moreData:\n",
    "                if numDataPoints%1000==0:\n",
    "                    print(\"Currently at data point \" + str(numDataPoints))\n",
    "                try:\n",
    "                    currData=[self.getNextDataPointSequential()]\n",
    "                    #dataClasses = self.getDataClasses(currData)#This could be removed to make it more effecient\n",
    "                    for att in attributes:\n",
    "                        if self.isDerived[att] != True:\n",
    "                            if self.isNominal[att]: #If it is nominal data\n",
    "                                if self.isSequence[att]:\n",
    "                                    raise(NotImplementedError(\"Nominal data types for sequences have not yet been implemented\"))\n",
    "                                dataClassLabels=self.getDataLabels(currData, att)\n",
    "                                if classLabels.get(att) is None: #If it is the first step\n",
    "                                    classLabels[att]=dataClassLabels\n",
    "                                else:\n",
    "                                    #print(np.concatenate(dataClassLabels,classLabels[datclass]))\n",
    "                                    classLabels[att]=np.unique(np.concatenate([dataClassLabels, classLabels[att]]))\n",
    "                            else:\n",
    "                                if self.isSequence[att]!=True:\n",
    "                                    #If is it nominal and not a sequence\n",
    "                                    raise(NotImplementedError(\"Non-nominal data types for non-sequences have not yet been implemented\"))\n",
    "                                else:\n",
    "                                    #Add to the variable running sum\n",
    "                                    tempData = currData[0]\n",
    "                                    #print(tempData.keys)\n",
    "                                    variableSums[att] += sum(tempData[att])\n",
    "                        else:\n",
    "                            #Handle the derived variables\n",
    "                            currentDerivedData = self.deriveData(att, currData[0])\n",
    "                            variableSums[att] += sum(currentDerivedData)\n",
    "\n",
    "                    numDataPoints=numDataPoints+1\n",
    "                except:\n",
    "                    moreData=False\n",
    "                    print(\"Stopped at \" + str(numDataPoints) + \" data points\")\n",
    "            \n",
    "            oneHotEncoders={}\n",
    "            encodingLengths={}\n",
    "            dataDim=0\n",
    "            for att in attributes:\n",
    "                if self.isSequence[att]==False:\n",
    "                    oneHotEncoders[att]=self.buildEncoder(classLabels[att])\n",
    "                    encodingLengths[att]=classLabels[att].size\n",
    "                    #dataDim=dataDim+encodingLengths[datclass]\n",
    "                else:\n",
    "                    if self.isNominal[att]:\n",
    "                        raise(NotImplementedError(\"Nominal data types for sequences have not yet been implemented\"))\n",
    "                    else:\n",
    "                        encodingLengths[att]=1\n",
    "                        #dataDim=dataDim+1\n",
    "            print(\"Getting data indices\")\n",
    "            dataPointIndices=jsonReader.getDataIndices(self.dataFileName)\n",
    "            \n",
    "            #Set all of the summary information to self properties\n",
    "            computeMeansStandardDeviations(self, variableSums, numDataPoints)\n",
    "            self.numDataPoints=numDataPoints\n",
    "            self.encodingLengths=encodingLengths#A dictionary that maps attributes to the lengths of their vector encoding schemes\n",
    "            self.oneHotEncoders=oneHotEncoders#A dictionary of dictionaries where the outer dictionary maps attributes to encoding schemes and where each encoding scheme is a dictionary that maps attribute values to one hot encodings\n",
    "            #self.dataDim=dataDim#The sum of all the encoding lengths for the relevant attributes\n",
    "            #self.isSequence=#A dictionary that returns whether an attribute takes the form of a sequence of data\n",
    "            #self.isNominal=#A dictionary that returns whether an attribute is nominal in form (neither numeric nor ordinal)\n",
    "            self.dataPointIndices=dataPointIndices\n",
    "            \n",
    "            #Save that summary file so that it can be used next time\n",
    "            self.writeSummaryFile()\n",
    "        self.MetaDataLoaded=True \n",
    "        \n",
    "    def computeMeansStandardDeviations(self, varSums, numDataPoints):\n",
    "        print(\"Computing variable means and standard deviations\")\n",
    "        \n",
    "        numSequencePoints = numDataPoints*500\n",
    "        variableMeans = {}\n",
    "        for key in varSums:\n",
    "            variableMeans[key] = varSums[key]/numSequencePoints\n",
    "        \n",
    "        varResidualSums = {'altitude':0, 'gender':0, 'heart_rate':0, 'id':0, 'latitude':0, 'longitude':0,\n",
    "                   'speed':0, 'sport':0, 'timestamp':0, 'url':0, 'userId':0, 'time_elapsed': 0,\n",
    "                            'distance':0, 'new_workout':0, 'derived_speed':0}\n",
    "        \n",
    "        self.dataFile=open(self.dataFileName, 'r')\n",
    "        moreData=True\n",
    "        while moreData:\n",
    "                if numDataPoints%10000==0:\n",
    "                    print(\"Currently at data point \" + str(numDataPoints))\n",
    "                try:\n",
    "                    currData=[self.getNextDataPointSequential()]\n",
    "                    #dataClasses = self.getDataClasses(currData)#This could be removed to make it more effecient\n",
    "                    for att in attributes:\n",
    "                        if self.isNominal[att] != True: #If it is nominal data\n",
    "                            if self.isSequence[att]!=True:\n",
    "                                #If is it nominal and not a sequence\n",
    "                                raise(NotImplementedError(\"Non-nominal data types for non-sequences have not yet been implemented\"))\n",
    "                            else:\n",
    "                                if self.isDerived[att] == True: #Handle derived variables\n",
    "                                    dataPointArray = np.array(self.deriveData(att, currData[0]))\n",
    "                                else:\n",
    "                                    dataPointArray = np.array(self.getDataLabels(currData, att))\n",
    "                                #Add to the variable running sum of squared residuals\n",
    "                                varResidualSums[att] += np.square(dataPointArray-variableMeans[att])\n",
    "                            \n",
    "                except:\n",
    "                    moreData=False\n",
    "                    print(\"Stopped at \" + str(numDataPoints) + \" data points\")\n",
    "        \n",
    "        variableStds = {}\n",
    "        for key in varResidualSums:\n",
    "            variableStds[key] = np.sqrt(varResidualSums[key]/numSequencePoints)\n",
    "            \n",
    "        self.variableMeans = variableMeans\n",
    "        self.variableStds = variableStds\n",
    "        \n",
    "        \n",
    "    def oneHot(self, dataPoint, att):\n",
    "        #Takes the current data point and the attribute type and uses the data schema to provide the one-hot encoding for the variable\n",
    "        dataValue=dataPoint[att]       \n",
    "        #Use a stored schema dictionary to return the correct encoding scheme for the attribute (an encoding scheme is also a dictionary)\n",
    "        encoder=self.oneHotEncoders[att]\n",
    "        #Use this encoding scheme to get the encoding\n",
    "        encoding=encoder[dataValue]\n",
    "        return encoding\n",
    "    \n",
    "    def invertOneHots(self):\n",
    "        #This function assumes that all values in a given encoder are unique\n",
    "        inverseOneHotEncoders={}\n",
    "        for i, att in enumerate(self.attributes):\n",
    "            if self.isSequence[att]==False:\n",
    "                currentEncoder=self.oneHotEncoders[att]\n",
    "                inverseOneHotEncoders[att] = {str(v): k for k, v in currentEncoder.iteritems()}\n",
    "        return inverseOneHotEncoders\n",
    "\n",
    "    def getInputDim(self, targetAtt):\n",
    "        return self.dataDim - self.encodingLengths[targetAtt]\n",
    "\n",
    "    def getTargetDim(self, targetAtt):\n",
    "        return self.encodingLengths[targetAtt]\n",
    "    \n",
    "    \n",
    "class metaDataEndomondo(object):\n",
    "    #For disk storage of metadata\n",
    "    #Meant to be pickled and unpickled\n",
    "    def __init__(self, numDataPoints, encodingLengths, oneHotEncoders, isSequence, isNominal, isDerived, dataPointIndices,\n",
    "                 variableMeans, variableStds):\n",
    "        self.numDataPoints=numDataPoints\n",
    "        self.encodingLengths=encodingLengths\n",
    "        self.oneHotEncoders=oneHotEncoders\n",
    "        #self.dataDim=dataDim\n",
    "        self.isSequence=isSequence\n",
    "        self.isNominal=isNominal\n",
    "        self.isDerived=isDerived\n",
    "        self.dataPointIndices=dataPointIndices\n",
    "        self.variableMeans = variableMeans\n",
    "        self.variableStds = variableStds\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
