{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import ijson\n",
    "import pickle\n",
    "import os\n",
    "import jsonReader\n",
    "from haversine import haversine\n",
    "\n",
    "#Need to:\n",
    "        #0. Scan the file when producing metadata and create a list of data point indices\n",
    "        #1. Load the list of data point indices\n",
    "        #2. Randomly permute the list of data point indices\n",
    "        #3. Split the list of data point indices into training, validation, and test sets\n",
    "        #4. Provide a method to get the next data point in the list (and either globally save the position of the list or save it implicitly in a generator)\n",
    "        #5. Provide a method to reset this list position (for a new epoch)\n",
    "        #6. Rewrite the endoIterator method to respect randomization (copy the random access iterator code from the ptb reader example)\n",
    "\n",
    "class dataInterpreter(object):\n",
    "    def __init__(self, fn=\"endomondoHR_proper.json\", attributes=None, allowMissingData=True, scaleVals=True, excisedFN = \"excisedData_auto_nohup.p\"):\n",
    "        self.dataFileName=fn#Will eventually replace this with a data folder name\n",
    "        self.dataFile=open(self.dataFileName, 'r')\n",
    "        self.MetaDataLoaded=False\n",
    "        self.dataSchemaLoaded=False\n",
    "        #self.currentDataPoint=None\n",
    "        self.dataPointPosition=0\n",
    "        self.attIgnore=['id','url','speed']#Attributes to ignore when building metadata\n",
    "        self.metaDataFn=fn[0:len(fn)-5]+\"_metaData.p\"\n",
    "        self.allowMissingData=allowMissingData\n",
    "        self.scaleVals=scaleVals\n",
    "        #self.valTestSplit=(.1,.1)\n",
    "        self.excisedFN = excisedFN\n",
    "        if attributes is not None:\n",
    "            self.buildDataSchema(attributes)\n",
    "    \n",
    "    def buildDataSchema(self, attributes, targetAtt, trainValTestSplit=(.8,.1,.1), zMultiple = 2):\n",
    "        self.targetAtt=targetAtt\n",
    "        self.buildMetaData()\n",
    "        self.splitForValidation(trainValTestSplit)\n",
    "        #self.newEpoch()#Reset all indices and counters\n",
    "        self.attributes=attributes\n",
    "        self.zMultiple = zMultiple\n",
    "        dataDimSum=0\n",
    "        for att in self.attributes:\n",
    "            dataDimSum=dataDimSum+self.encodingLengths[att]\n",
    "        self.dataDim=dataDimSum\n",
    "        \n",
    "        #Create a dictionary that takes an attribute and returns a beginning and end position of the attribute in the input sequence\n",
    "        self.inputAttributes =[x for x in self.attributes if x != targetAtt]\n",
    "        self.inputIndices={}\n",
    "        lastIndex=0\n",
    "        for att in self.inputAttributes:\n",
    "            nextIndex=lastIndex+self.encodingLengths[att]\n",
    "            self.inputIndices[att]=(lastIndex, nextIndex)\n",
    "            lastIndex=nextIndex\n",
    "            \n",
    "        self.dataSchemaLoaded=True\n",
    "    \n",
    "    def createSequentialGenerator(self): #Define a new data generator\n",
    "        filename = self.dataFileName\n",
    "        self.f=open(filename, 'r')\n",
    "        objects = ijson.items(self.f, 'users.item')\n",
    "        self.dataObjects=objects\n",
    "        return self.dataObjects\n",
    "    \n",
    "    def dataGenerator(self, dataSetOrder):\n",
    "        for dp_index in dataSetOrder:\n",
    "            fileIndices = self.dataPointIndices[dp_index]\n",
    "            potentialNextDataPoint=jsonReader.getDataPoint(fileIndices, self.dataFile)\n",
    "            \n",
    "            \"\"\"\n",
    "            if self.allowMissingData==False:\n",
    "                #Check if the next data point contains all the requested attributes\n",
    "                for i, att in enumerate(self.attributes):\n",
    "                    #print(att)\n",
    "                    try:\n",
    "                        test=self.currentDataPoint[att]\n",
    "                    except:\n",
    "                        print(\"Skipping data point because it lacks attribute: \" + att)\n",
    "                        #print(\"Skipping data point because it lacks attribute\")\n",
    "                        return self.getNextDataPoint() #Try the next one instead\n",
    "            \"\"\"\n",
    "            yield potentialNextDataPoint #returns next data point\n",
    "    \n",
    "    def randomizeDataOrder(self, dataIndices):\n",
    "        return np.random.permutation(dataIndices)\n",
    "\n",
    "    #def getNextDataPoint(self):\n",
    "    #        jsonReader.getDataPoint(index, dataFile)\n",
    "    #    return dataPoint\n",
    "\n",
    "    def getNextDataPointSequential(self):\n",
    "        try: #If there is a generator already defined\n",
    "            objects=self.dataObjects\n",
    "        except: #Otherwise create a new one\n",
    "            #Creating new generator\n",
    "            objects=self.createSequentialGenerator()\n",
    "        nextDataPoint=self.__convert(objects.next())\n",
    "\n",
    "        return nextDataPoint\n",
    "    \n",
    "    #def newEpoch(self):\n",
    "        # A convenience function for reseting the data loader to start a new epoch\n",
    "        #self.currentDataPoint = None\n",
    "    #    self.dataPointPosition = 0  # The position within a data point (within an exercise)       \n",
    "    \n",
    "    \n",
    "    def batchIteratorSupervised(self, batch_size, trainValidTest, targetAtt):\n",
    "        #Performs the same job as the batch iterator, but with one of the attributes separated as the supervision signal\n",
    "        inputAttributes = self.inputAttributes\n",
    "        inputDataDim=self.getInputDim(targetAtt)\n",
    "        targetDataDim=self.getTargetDim(targetAtt)\n",
    "\n",
    "        if trainValidTest == 'train':\n",
    "            self.trainingOrder = self.randomizeDataOrder(self.trainingSet)\n",
    "            dataGen = self.dataGenerator(self.trainingOrder)\n",
    "        elif trainValidTest == 'valid':\n",
    "            self.validationOrder = self.randomizeDataOrder(self.validationSet)\n",
    "            dataGen = self.dataGenerator(self.validationOrder)\n",
    "        elif trainValidTest == 'test':\n",
    "            self.testOrder = self.randomizeDataOrder(self.testSet)\n",
    "            dataGen = self.dataGenerator(self.testOrder)\n",
    "        else:\n",
    "            raise (Exception(\"Invalid dataset type. Must be 'train', 'valid', or 'test'\"))\n",
    "\n",
    "        if self.dataSchemaLoaded == False:\n",
    "            raise (RuntimeError(\"Need to load a data schema\"))\n",
    "\n",
    "        inputDataBatch = np.zeros((batch_size, inputDataDim))\n",
    "        # inputDataDim is the total concatenated length of the data at each time point (for all input attributes)\n",
    "        targetDataBatch = np.zeros((batch_size, targetDataDim))\n",
    "\n",
    "\n",
    "        # if currentDataPoint is None: #If starting an epoch, grab the first data point\n",
    "        currentDataPoint = dataGen.next()\n",
    "        dataPointPosition = 0\n",
    "        currentDataPointLength = self.getDataPointLength(currentDataPoint)\n",
    "        moreData = True\n",
    "        currentDerivedData={}\n",
    "        while moreData:\n",
    "            for i in range(batch_size):\n",
    "                # Need code for getting the current data point and iterating through it until the end of it...\n",
    "                # if end of data point:\n",
    "                # currentPoint = next data point\n",
    "                dataList = []  # A mutable data structure to allow us to construct the data instance...\n",
    "                if dataPointPosition == currentDataPointLength:  # Check to see if new data point is needed\n",
    "                    try:\n",
    "                        currentDataPoint = dataGen.next()\n",
    "                    except:  # If there is no more data, return what you have\n",
    "                        moreData = False\n",
    "                        yield [inputDataBatch, targetDataBatch]  # May need to pad this??\n",
    "                    currentDataPointLength = self.getDataPointLength(currentDataPoint)\n",
    "                    currentDerivedData = {} #Reset the derived data dictionary\n",
    "                    dataPointPosition = 0\n",
    "                for j, att in enumerate(inputAttributes):\n",
    "                    if self.isDerived[att]:\n",
    "                        #handle the derived variables\n",
    "                        if att in currentDerivedData.keys():\n",
    "                            #Use the data from the current data point position\n",
    "                            attDataPoint=currentDerivedData[att]\n",
    "                            attData = attDataPoint[dataPointPosition]\n",
    "                        else:\n",
    "                            #Generate the data and then use the data from the current data point position which should be 0\n",
    "                            currentDerivedData[att] = self.deriveData(att, currentDataPoint)\n",
    "                            attDataPoint=currentDerivedData[att]\n",
    "                            attData = attDataPoint[dataPointPosition]\n",
    "                    else:\n",
    "                        if self.isSequence[att]:  # Need to limit the sequence to the end of the batch...\n",
    "                            # Put the sequence attributes in their proper positions in the tensor array\n",
    "                            # These are numeric encoding schemes.\n",
    "                            attData = currentDataPoint[att][\n",
    "                                dataPointPosition]  # Get the next entry in the attribute sequence for the current data point\n",
    "                        else:\n",
    "                            # Put the context attributes in their proper positions in the tensor array\n",
    "                            # These are a one-hot encoding schemes except in the case of \"age\" and the like\n",
    "                            if self.isNominal[att]:  # Checks whether the data is nominal\n",
    "                                attData = self.oneHot(currentDataPoint, att)  # returns a list\n",
    "                            else:\n",
    "                                attData = currentDataPoint  # Handles ordinal and numeric data\n",
    "\n",
    "                    scaledAttData = self.scaleData(attData, att, self.zMultiple)  # Rescales data if needed\n",
    "                    if self.isList(scaledAttData):\n",
    "                        dataList.extend(scaledAttData)\n",
    "                    else:\n",
    "                        dataList.append(scaledAttData)\n",
    "                #Now do the same for the target attribute\n",
    "                if self.isSequence[targetAtt]:  # Need to limit the sequence to the end of the batch...\n",
    "                    # Put the target attribute in its proper positions in the tensor array\n",
    "                    # These are numeric encoding schemes.\n",
    "                    attData = currentDataPoint[targetAtt][\n",
    "                        dataPointPosition]  # Get the next entry in the attribute sequence for the current data point\n",
    "                else:\n",
    "                    # Put the context attributes in their proper positions in the tensor array\n",
    "                    # These are a one-hot encoding schemes except in the case of \"age\" and the like\n",
    "                    if self.isNominal[targetAtt]:  # Checks whether the data is nominal\n",
    "                        attData = self.oneHot(currentDataPoint, targetAtt)  # returns a list\n",
    "                    else:\n",
    "                        attData = currentDataPoint  # Handles ordinal and numeric data\n",
    "\n",
    "                scaledTargetAttData = self.scaleData(attData, targetAtt, self.zMultiple)  # Rescales data if needed\n",
    "                targetDataBatch[i,:] = scaledTargetAttData#Add the target data for the current data point to the full list\n",
    "\n",
    "                #Check length of input data vector\n",
    "                if len(dataList) == inputDataDim:\n",
    "                    inputDataBatch[i, :] = dataList\n",
    "                else:\n",
    "                    print(\"Data list length: \" + str(len(dataList)))\n",
    "                    print(\"Data schema length: \" + str(len(inputDataDim)))\n",
    "                    raise (ValueError(\"Data is not formatted according to the schema\"))\n",
    "\n",
    "                dataPointPosition = dataPointPosition + 1\n",
    "\n",
    "            yield [inputDataBatch, targetDataBatch]\n",
    "            \n",
    "    def dataIteratorSupervised(self, trainValidTest):\n",
    "        targetAtt=self.targetAtt\n",
    "        #Performs the same job as the batch iterator, but with one of the attributes separated as the supervision signal\n",
    "        inputAttributes = self.inputAttributes\n",
    "        inputDataDim=self.getInputDim(targetAtt)\n",
    "        targetDataDim=self.getTargetDim(targetAtt)\n",
    "\n",
    "\n",
    "        if trainValidTest == 'train':\n",
    "            self.trainingOrder = self.randomizeDataOrder(self.trainingSet)\n",
    "            dataGen = self.dataGenerator(self.trainingOrder)\n",
    "        elif trainValidTest == 'valid':\n",
    "            self.validationOrder = self.randomizeDataOrder(self.validationSet)\n",
    "            dataGen = self.dataGenerator(self.validationOrder)\n",
    "        elif trainValidTest == 'test':\n",
    "            self.testOrder = self.randomizeDataOrder(self.testSet)\n",
    "            dataGen = self.dataGenerator(self.testOrder)\n",
    "        else:\n",
    "            raise (Exception(\"Invalid dataset type. Must be 'train', 'valid', or 'test'\"))\n",
    "\n",
    "        if self.dataSchemaLoaded == False:\n",
    "            raise (RuntimeError(\"Need to load a data schema\"))\n",
    "\n",
    "        #inputData = np.zeros((batch_size, inputDataDim))\n",
    "        # inputDataDim is the total concatenated length of the data at each time point (for all input attributes)\n",
    "        #targetData = np.zeros((batch_size, targetDataDim))\n",
    "\n",
    "\n",
    "        # if currentDataPoint is None: #If starting an epoch, grab the first data point\n",
    "        currentDataPoint = dataGen.next()\n",
    "        dataPointPosition = 0\n",
    "        currentDataPointLength = self.getDataPointLength(currentDataPoint)\n",
    "        moreData = True\n",
    "        currentDerivedData={}\n",
    "        while moreData:\n",
    "            # Need code for getting the current data point and iterating through it until the end of it...\n",
    "            # if end of data point:\n",
    "            # currentPoint = next data point\n",
    "            dataList = []  # A mutable data structure to allow us to construct the data instance...\n",
    "            if dataPointPosition == currentDataPointLength:  # Check to see if new data point is needed\n",
    "                try:\n",
    "                    currentDataPoint = dataGen.next()\n",
    "                except:  # If there is no more data, return what you have\n",
    "                    moreData = False\n",
    "                    yield [inputData, targetData]  # May need to pad this??\n",
    "                currentDataPointLength = self.getDataPointLength(currentDataPoint)\n",
    "                currentDerivedData = {} #Reset the derived data dictionary\n",
    "                dataPointPosition = 0\n",
    "            for j, att in enumerate(inputAttributes):\n",
    "                if self.isDerived[att]:\n",
    "                    #handle the derived variables\n",
    "                    if att in currentDerivedData.keys():\n",
    "                        #Use the data from the current data point position\n",
    "                        attDataPoint=currentDerivedData[att]\n",
    "                        attData = attDataPoint[dataPointPosition]\n",
    "                    else:\n",
    "                        #Generate the data and then use the data from the current data point position which should be 0\n",
    "                        currentDerivedData[att] = self.deriveData(att, currentDataPoint)\n",
    "                        attDataPoint=currentDerivedData[att]\n",
    "                        attData = attDataPoint[dataPointPosition]\n",
    "                else:\n",
    "                    if self.isSequence[att]:  # Need to limit the sequence to the end of the batch...\n",
    "                        # Put the sequence attributes in their proper positions in the tensor array\n",
    "                        # These are numeric encoding schemes.\n",
    "                        attData = currentDataPoint[att][\n",
    "                            dataPointPosition]  # Get the next entry in the attribute sequence for the current data point\n",
    "                    else:\n",
    "                        # Put the context attributes in their proper positions in the tensor array\n",
    "                        # These are a one-hot encoding schemes except in the case of \"age\" and the like\n",
    "                        if self.isNominal[att]:  # Checks whether the data is nominal\n",
    "                            attData = self.oneHot(currentDataPoint, att)  # returns a list\n",
    "                        else:\n",
    "                            attData = currentDataPoint  # Handles ordinal and numeric data\n",
    "\n",
    "                scaledAttData = self.scaleData(attData, att, self.zMultiple)  # Rescales data if needed\n",
    "                if self.isList(scaledAttData):\n",
    "                    dataList.extend(scaledAttData)\n",
    "                else:\n",
    "                    dataList.append(scaledAttData)\n",
    "            #Now do the same for the target attribute\n",
    "            if self.isSequence[targetAtt]:  # Need to limit the sequence to the end of the batch...\n",
    "                # Put the target attribute in its proper positions in the tensor array\n",
    "                # These are numeric encoding schemes.\n",
    "                attTarData = currentDataPoint[targetAtt][\n",
    "                    dataPointPosition]  # Get the next entry in the attribute sequence for the current data point\n",
    "            else:\n",
    "                # Put the context attributes in their proper positions in the tensor array\n",
    "                # These are a one-hot encoding schemes except in the case of \"age\" and the like\n",
    "                if self.isNominal[targetAtt]:  # Checks whether the data is nominal\n",
    "                    attTarData = self.oneHot(currentDataPoint, targetAtt)  # returns a list\n",
    "                else:\n",
    "                    attTarData = currentDataPoint  # Handles ordinal and numeric data\n",
    "\n",
    "            scaledTargetAttData = self.scaleData(attTarData, targetAtt, self.zMultiple)  # Rescales data if needed\n",
    "            targetData = [] #So that we can return it in the same list format as the inputs\n",
    "            targetData.append(scaledTargetAttData)#Add the target data for the current data point to the full list\n",
    "\n",
    "            #Check length of input data vector\n",
    "            if len(dataList) == inputDataDim:\n",
    "                inputData = dataList\n",
    "            else:\n",
    "                print(\"Data list length: \" + str(len(dataList)))\n",
    "                print(\"Data schema length: \" + str(len(inputDataDim)))\n",
    "                raise (ValueError(\"Data is not formatted according to the schema\"))\n",
    "\n",
    "            dataPointPosition = dataPointPosition + 1\n",
    "\n",
    "            yield [inputData, targetData]\n",
    "\n",
    "\n",
    "    def batchIterator(self, batch_size, trainValidTest):\n",
    "        #Returns a tensorflow tensor (a numpy array) containing a batch of data\n",
    "        #Can be used directly for feed or to preprocess for additional efficiency\n",
    "        \n",
    "        #Currently does not explicitly separate exercise routines. \n",
    "        #Can be augmented with a variable that captures end and begnning of a routine if this helps.\n",
    "        \n",
    "        if trainValidTest=='train':\n",
    "            self.trainingOrder = self.randomizeDataOrder(self.trainingSet)\n",
    "            dataGen=self.dataGenerator(self.trainingOrder)\n",
    "        elif trainValidTest=='valid':\n",
    "            self.validationOrder = self.randomizeDataOrder(self.validationSet)\n",
    "            dataGen=self.dataGenerator(self.validationOrder)\n",
    "        elif trainValidTest=='test':\n",
    "            self.testOrder = self.randomizeDataOrder(self.testSet)\n",
    "            dataGen=self.dataGenerator(self.testOrder)\n",
    "        else:\n",
    "            raise(Exception(\"Invalid dataset type. Must be 'train', 'valid', or 'test'\"))\n",
    "        \n",
    "        if self.dataSchemaLoaded==False:\n",
    "            raise(RuntimeError(\"Need to load a data schema\"))\n",
    "        \n",
    "        dataBatch = np.zeros((batch_size, self.dataDim))\n",
    "        #self.dataDim is the total concatenated length of the data at each time point (for all attributes)\n",
    "                \n",
    "        #if currentDataPoint is None: #If starting an epoch, grab the first data point\n",
    "        currentDataPoint=dataGen.next()\n",
    "        dataPointPosition=0\n",
    "        currentDataPointLength=self.getDataPointLength(currentDataPoint)\n",
    "        moreData=True\n",
    "        while moreData:\n",
    "            for i in range(batch_size):\n",
    "                #Need code for getting the current data point and iterating through it until the end of it...\n",
    "                #if end of data point:\n",
    "                    #currentPoint = next data point\n",
    "                dataList = [] #A mutable data structure to allow us to construct the data instance...\n",
    "                if dataPointPosition==currentDataPointLength: #Check to see if new data point is needed\n",
    "                    try:\n",
    "                        currentDataPoint=dataGen.next()\n",
    "                    except: #If there is no more data, return what you have\n",
    "                        moreData=False\n",
    "                        yield dataBatch #May need to pad this??\n",
    "                    currentDataPointLength=self.getDataPointLength(currentDataPoint)\n",
    "                    dataPointPosition=0\n",
    "                for j, att in enumerate(self.attributes):\n",
    "                    if self.isSequence[att]: #Need to limit the sequence to the end of the batch...\n",
    "                        #Put the sequence attributes in their proper positions in the tensor array\n",
    "                        #These are numeric encoding schemes.\n",
    "                        attData=currentDataPoint[att][dataPointPosition]#Get the next entry in the attribute sequence for the current data point\n",
    "                    else:\n",
    "                        #Put the context attributes in their proper positions in the tensor array\n",
    "                        #These are a one-hot encoding schemes except in the case of \"age\" and the like\n",
    "                        if self.isNominal[att]:#Checks whether the data is nominal\n",
    "                            attData = self.oneHot(currentDataPoint, att) #returns a list\n",
    "                        else:\n",
    "                            attData = currentDataPoint #Handles ordinal and numeric data\n",
    "\n",
    "                    scaledAttData=self.scaleData(attData, att, self.zMultiple)#Rescales data if needed\n",
    "                    if self.isList(scaledAttData):\n",
    "                        dataList.extend(scaledAttData)\n",
    "                    else:\n",
    "                        dataList.append(scaledAttData)           \n",
    "                if len(dataList)==self.dataDim:\n",
    "                    dataBatch[i,:]=dataList\n",
    "                else:\n",
    "                    print(\"Data list length: \" + dataList)\n",
    "                    print(\"Data schema length: \" + self.dataDim)\n",
    "                    raise(ValueError(\"Data is not formatted according to the schema\"))\n",
    "\n",
    "                dataPointPosition=dataPointPosition+1\n",
    "\n",
    "            yield dataBatch\n",
    "\n",
    "    def endoIteratorSupervised(self, batch_size, num_steps, trainValidTest):\n",
    "            #Does the same thing as the endoIterator, except for a model with separate supervised targets (targets are not the next element in the sequence)\n",
    "\n",
    "            batchGen = self.dataIteratorSupervised(trainValidTest)\n",
    "\n",
    "            data_len = self.numDataPoints\n",
    "            batch_len = data_len // batch_size\n",
    "            epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "            inputDataDim = self.getInputDim(self.targetAtt)\n",
    "            targetDataDim = self.getTargetDim(self.targetAtt)\n",
    "\n",
    "            if epoch_size == 0:\n",
    "                raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "                \n",
    "\n",
    "            nextRow=[next(batchGen) for x in range(num_steps)]#Fill the first row (with both inputs and targets)\n",
    "            #print(nextRowTargets)\n",
    "            for i in range(epoch_size):\n",
    "                inputData = np.zeros([batch_size*num_steps, inputDataDim])\n",
    "                targetData = np.zeros([batch_size*num_steps, targetDataDim])\n",
    "                for j in range(batch_size*num_steps):\n",
    "                    currentTimePoint = next(batchGen)\n",
    "                    inputData[j,:] = currentTimePoint[0]\n",
    "                    targetData[j,:] = currentTimePoint[1]\n",
    "\n",
    "                x = inputData\n",
    "                y = targetData\n",
    "                yield (x, y)\n",
    "\n",
    "    \n",
    "    def endoIterator(self, batch_size, num_steps, trainValidTest):\n",
    "        #Returns a batch-wise generator for the endomondo data with inputs as the current element in the sequence and targets as the next element in the sequence\n",
    "\n",
    "        batchGen = self.batchIterator(batch_size*(num_steps+1), trainValidTest)\n",
    "\n",
    "        data_len = self.numDataPoints\n",
    "        batch_len = data_len // batch_size\n",
    "        epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "        if epoch_size == 0:\n",
    "            raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "        #For these guys, the labels are simply the next sequence. This is to train the model to reprodue the text.\n",
    "        #Since I am not really trying to do this, I should generate the labels seperately.\n",
    "        #However, I might find that training the net this way (to predict the sequence) and then transplanting the weights into the full model might be useful...\n",
    "        \"\"\"for i in range(epoch_size):\n",
    "            batchData=self.nextBatch(batch_size)\n",
    "            data = np.zeros([batch_size, batch_len, self.dataDim])\n",
    "            for j in range(batch_size):\n",
    "                data[j] = batchData[batch_len * j:batch_len * (j + 1)]\n",
    "            x = data[:, i*num_steps:(i+1)*num_steps]\n",
    "            y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "            yield (x, y)\"\"\"\n",
    "        \n",
    "        #The code below is not ideal because it trains everything in order whereby each batch is comprised sequentially of the data\n",
    "        #It should be OK for basic testing, however.\n",
    "        #and it may miss some transitions (not sure)\n",
    "        #print( epoch_size)\n",
    "        for i in range(epoch_size):\n",
    "            batchData=batchGen.next()\n",
    "            #print(batchData.shape)\n",
    "            data = np.zeros([batch_size, num_steps+1, self.dataDim])\n",
    "            for j in range(batch_size):\n",
    "                data[j,:,:] = batchData[(num_steps * j):((num_steps * (j + 1))+1),:]\n",
    "            #print(data.shape)\n",
    "            #x = data[:, i*num_steps:((i+1)*num_steps),:]\n",
    "            x = data[:, 0:num_steps, :]\n",
    "            #y = data[:, i*(num_steps+1):((i+1)*num_steps+1),:]\n",
    "            y = data[:, 1:(num_steps+1), :]\n",
    "            yield (x, y)\n",
    "        \n",
    "    \n",
    "    def splitForValidation(self, valTestSplit):\n",
    "        #Construct seperate data files for the training, test, and validation data\n",
    "        \n",
    "        #First excise the excised list\n",
    "        self.excisedList = self.loadExcisedList(self.excisedFN)\n",
    "        self.enalteredNumDPs = self.numDataPoints\n",
    "        self.numDataPoints = self.enalteredNumDPs-len(self.excisedList)\n",
    "        excisedSet = set(self.excisedList)\n",
    "        self.dataPointList = [x for x in range(self.enalteredNumDPs) if x not in excisedSet]\n",
    "        \n",
    "        trainingSetSize=int(round(self.numDataPoints*valTestSplit[0]))\n",
    "        validationSetSize=int(round(self.numDataPoints*valTestSplit[1]))\n",
    "        testSetSize=int(round(self.numDataPoints*valTestSplit[2]))\n",
    "        randomOrder=self.randomizeDataOrder(self.dataPointList)\n",
    "        \n",
    "        self.trainingSet=randomOrder[0:trainingSetSize]\n",
    "        self.validationSet=randomOrder[trainingSetSize:trainingSetSize+validationSetSize]\n",
    "        self.testSet=randomOrder[trainingSetSize+validationSetSize:trainingSetSize+validationSetSize+testSetSize]\n",
    "        \n",
    "        #print(\"training set size:\" + str(len(self.trainingSet))\n",
    "        #print(\"validation set size:\" + str(len(self.validationSet))\n",
    "        #print(\"test set size:\" + str(len(self.testSet)))\n",
    "        \n",
    "    def loadExcisedList(self, excisedFN):\n",
    "        with open(excisedFN, \"rb\") as f:\n",
    "            exc_l = pickle.load(f)\n",
    "        return exc_l\n",
    "    \n",
    "    def decoderKey(self):\n",
    "        return [x for x in self.attributes if x != targetAtt]\n",
    "    \n",
    "    def dataDecoder(self, dataPoints):\n",
    "        convertedData=[]\n",
    "        for dp in dataPoints:\n",
    "            convertedData.append(self.dataDecoderDP(dp))\n",
    "        return convertedData\n",
    "    \n",
    "    def dataDecoderDP(self, dataPoint):\n",
    "        #This function takes an encoded data point (a single time step) and decodes it into a readable set of variables \n",
    "        #for use in statistical processing and visualization\n",
    "        inputAttributes = [x for x in self.attributes if x != self.targetAtt]\n",
    "        #inputDataDim=self.getInputDim(inputAttributes)\n",
    "        \n",
    "        try:\n",
    "            inverseEncoders=self.inverseOneHotEncoders\n",
    "        except:\n",
    "            self.inverseOneHotEncoders=self.invertOneHots()\n",
    "            inverseEncoders=self.inverseOneHotEncoders\n",
    "        \n",
    "        decodedDataPoint=[]\n",
    "        dataPointPosition=0\n",
    "        for i, att in enumerate(inputAttributes):\n",
    "            attLength = self.encodingLengths[att]\n",
    "            currentAttData = dataPoint[dataPointPosition:dataPointPosition+attLength]\n",
    "            dataPointPosition = dataPointPosition + attLength\n",
    "            if self.isSequence[att]==False:\n",
    "                currentEncoder=inverseEncoders[att]\n",
    "                decodedDataPoint.append(currentEncoder[str([int(i) for i in list(currentAttData)])])\n",
    "            else:\n",
    "                decodedDataPoint.append(currentAttData[0])\n",
    "\n",
    "        return decodedDataPoint\n",
    "\n",
    "    def deriveData(self, att, currentDataPoint):\n",
    "        if att=='time_elapsed':\n",
    "            #Derive the time elapsed data sequence\n",
    "            timestamps=currentDataPoint['timestamp']\n",
    "            initialTime=timestamps[0]\n",
    "            return [x-initialTime for x in timestamps]\n",
    "            #Going to need to scale these appropriately in the scaling function\n",
    "        elif att=='distance':\n",
    "            #Derive the distance data sequence\n",
    "            lats=currentDataPoint['latitude']\n",
    "            longs=currentDataPoint['longitude']\n",
    "            indices=range(1, len(lats)) #Creates a list of indices from 1 to the length of the seuqnces to index all elements but the first (element zero)\n",
    "            distances=[0]\n",
    "            distances.extend([haversine([lats[i-1],longs[i-1]], [lats[i],longs[i]]) for i in indices]) #Gets distance traveled since last time point in kilometers\n",
    "            return distances\n",
    "        elif att=='new_workout':\n",
    "            workoutLength=len(currentDataPoint['timestamp'])#This is likely always equal to 500\n",
    "            newWorkout=np.zeros(workoutLength)\n",
    "            newWorkout[0]=1 #Add the signal\n",
    "            return newWorkout\n",
    "        elif att=='derived_speed':\n",
    "            distances=self.deriveData('distance', currentDataPoint)\n",
    "            timestamps=currentDataPoint['timestamp']\n",
    "            indices=range(1, len(timestamps))\n",
    "            times=[0]\n",
    "            times.extend([timestamps[i] - timestamps[i-1] for i in indices])\n",
    "            derivedSpeeds=[0]\n",
    "            try:\n",
    "                derivedSpeeds.extend([distances[i]/times[i] for i in indices])\n",
    "            except:\n",
    "                #print(\"Exception in deriving speeds\")\n",
    "                for i in indices:\n",
    "                    try:\n",
    "                        derivedSpeeds.append(distances[i]/times[i])\n",
    "                    except:\n",
    "                        derivedSpeeds.append(0)\n",
    "            return derivedSpeeds\n",
    "        else:\n",
    "            raise(Exception(\"No such derived data attribute\"))\n",
    "            \n",
    "    \"\"\"def scaleData(self, data, att):\n",
    "        #This function provides optional rescaling of the data for optimal neural network performance. \n",
    "        #It can either be run online or offline w/ results stored in a preprocessed data file (more effecient)\n",
    "        if self.scaleVals:\n",
    "            if att==\"speed\":\n",
    "                scaledData=data\n",
    "                return scaledData\n",
    "            elif att==\"heart_rate\":\n",
    "                scaledData=data/250.0 #This will be replaced with an auto-ranging version\n",
    "                return scaledData\n",
    "            elif att==\"altitude\":\n",
    "                scaledData=float(data)/10000.0 #This will be replaced with an auto-ranging version\n",
    "                return scaledData\n",
    "            else:\n",
    "                return data\n",
    "        else:\n",
    "            return data\"\"\"\n",
    "        \n",
    "    def scaleData(self, data, att, zMultiple=2):\n",
    "        #This function scales the data based on precomputed means and standard deviations\n",
    "        #It does this by computing z-scores and multiplying them based on a scaling paramater\n",
    "        #It therefore produces zero-centered data, which is important for the drop-in procedure\n",
    "        if self.scaleVals:\n",
    "            if self.isSequence[att]:\n",
    "                diff = data-float(self.variableMeans[att])\n",
    "                zScore = diff/float(self.variableStds[att])\n",
    "                return zScore * zMultiple\n",
    "            else:\n",
    "                return data\n",
    "        else:\n",
    "            return data\n",
    "        \n",
    "    def __convert(self, unicData): #Converts the unicode text in a dictionary to ascii\n",
    "        #Shamelessly lifted from http://stackoverflow.com/questions/13101653/python-convert-complex-dictionary-of-strings-from-unicode-to-ascii\n",
    "        if isinstance(unicData, dict):\n",
    "            return {self.__convert(key): self.__convert(value) for key, value in unicData.iteritems()}\n",
    "        elif isinstance(unicData, list):\n",
    "            return [self.__convert(element) for element in unicData]\n",
    "        elif isinstance(unicData, unicode):\n",
    "            return unicData.encode('utf-8')\n",
    "        else:\n",
    "            return unicData\n",
    "        \n",
    "    def getDataPointLength(self, dataPoint):\n",
    "        #Checks a single attribute. If the length of all sequence attributes is not equal, additional code will need to be written...\n",
    "        return len(dataPoint[\"heart_rate\"])#tries \"heart_rate\"\n",
    "    \n",
    "    def isList(self, attData):\n",
    "        #checks whether the variable attData is a list and returns true or false\n",
    "        return isinstance(attData, list)\n",
    "        #might want to try isSubclass(attData, list) if this doesn't work...\n",
    "    \n",
    "    def buildEncoder(self, classLabels):\n",
    "        #Constructs a dictionary that maps each class label to a list (encoding scheme) where one entry in the list is 1 and the remainder are 0\n",
    "        encodingLength=classLabels.size\n",
    "        encoder={}\n",
    "        for i, label in enumerate(classLabels):\n",
    "            encoding=[0] * encodingLength\n",
    "            encoding[i]=1\n",
    "            encoder[label]=encoding\n",
    "        return encoder\n",
    "    \n",
    "    def getDataLabels(self, data, dataClass):\n",
    "        #The \"data\" argument is in the same format as is returned by \"getNdatapoints\"\n",
    "        #If there is a use case that involves finding all the possible labels for a given class, a seperate function should be written to save memory usage...\n",
    "        class_labels = [col[dataClass] for col in data]\n",
    "        return np.unique(np.array(class_labels))\n",
    "    \n",
    "    def writeSummaryFile(self):\n",
    "        metaDataForWriting=metaDataEndomondo(self.numDataPoints, self.encodingLengths, self.oneHotEncoders, self.isSequence, \n",
    "                                             self.isNominal, self.isDerived, self.dataPointIndices, self.variableMeans, self.variableStds)\n",
    "        with open(self.metaDataFn, \"wb\") as f:\n",
    "            pickle.dump(metaDataForWriting, f)\n",
    "\n",
    "        #pickle.dump(metaDataForWriting, open(self.metaDataFn, \"wb\"))\n",
    "        print(\"Summary file written\")\n",
    "        \n",
    "    def loadSummaryFile(self):\n",
    "        try:\n",
    "            print(\"Loading metadata\")\n",
    "            with open(self.metaDataFn, \"rb\") as f:\n",
    "                metaData = pickle.load(f)\n",
    "                \n",
    "            #metaData=pickle.load(open(self.metaDataFn, \"rb\"))\n",
    "        except:\n",
    "            raise(IOError(\"Metadata file: \" + self.metaDataFn + \" not in valid pickle format\"))\n",
    "        self.numDataPoints=metaData.numDataPoints\n",
    "        self.encodingLengths=metaData.encodingLengths\n",
    "        self.oneHotEncoders=metaData.oneHotEncoders\n",
    "        #self.dataDim=metaData.dataDim\n",
    "        self.isSequence=metaData.isSequence\n",
    "        self.isNominal=metaData.isNominal\n",
    "        self.isDerived=metaData.isDerived\n",
    "        self.dataPointIndices=metaData.dataPointIndices\n",
    "        self.variableMeans = metaData.variableMeans\n",
    "        self.variableStds = metaData.variableStds\n",
    "        print(\"Metadata loaded\")\n",
    "        \n",
    "    def buildMetaData(self):\n",
    "        #Takes a list of attributes and the current datafile and constructs a schema for the data to be input into the RNN.\n",
    "        if os.path.isfile(self.metaDataFn):#If a summary file exists\n",
    "            self.loadSummaryFile()#Load that summary file and use it to capture all the necessary info\n",
    "        else:\n",
    "            print(\"Building data schema\")\n",
    "            #Build such a summary file by running through the full dataset and capturing the necessary statistics\n",
    "            self.isSequence={'altitude':True, 'gender':False, 'heart_rate':True, 'id':False, 'latitude':True, 'longitude':True,\n",
    "                             'speed':True, 'sport':False, 'timestamp':True, 'url':False, 'userId':False, 'time_elapsed': True, \n",
    "                             'distance':True, 'new_workout':True, 'derived_speed':True}#Handcoded\n",
    "            self.isNominal={'altitude':False, 'gender':True, 'heart_rate':False, 'id':True, 'latitude':False, 'longitude':False,\n",
    "                            'speed':False, 'sport':True, 'timestamp':False, 'url':True, 'userId':True, 'time_elapsed': False,\n",
    "                            'distance':False, 'new_workout':False, 'derived_speed':False}#Handcoded\n",
    "            self.isDerived={'altitude':False, 'gender':False, 'heart_rate':False, 'id':False, 'latitude':False, 'longitude':False,\n",
    "                   'speed':False, 'sport':False, 'timestamp':False, 'url':False, 'userId':False, 'time_elapsed': True,\n",
    "                            'distance':True, 'new_workout':True, 'derived_speed':True}#Handcoded\n",
    "            allAttributes=['altitude', 'gender', 'heart_rate', 'id', 'latitude', 'longitude',\n",
    "                   'speed', 'sport', 'timestamp', 'url', 'userId', 'time_elapsed', 'distance', 'new_workout', 'derived_speed']\n",
    "            attributes=[x for x in allAttributes if x not in self.attIgnore]#get rid of the attributes that we are ignoring\n",
    "            \n",
    "            variableSums={'altitude':0, 'gender':0, 'heart_rate':0, 'id':0, 'latitude':0, 'longitude':0,\n",
    "                   'speed':0, 'sport':0, 'timestamp':0, 'url':0, 'userId':0, 'time_elapsed': 0,\n",
    "                            'distance':0, 'new_workout':0, 'derived_speed':0}\n",
    "            \n",
    "            #self.newEpoch()#makes sure to reset things\n",
    "            moreData=True\n",
    "            classLabels={}\n",
    "            numDataPoints=0\n",
    "            while moreData:\n",
    "                if numDataPoints%1000==0:\n",
    "                    print(\"Currently at data point \" + str(numDataPoints))\n",
    "                try:\n",
    "                    currData=[self.getNextDataPointSequential()]\n",
    "                    #dataClasses = self.getDataClasses(currData)#This could be removed to make it more effecient\n",
    "                    for att in attributes:\n",
    "                        if self.isDerived[att] != True:\n",
    "                            if self.isNominal[att]: #If it is nominal data\n",
    "                                if self.isSequence[att]:\n",
    "                                    raise(NotImplementedError(\"Nominal data types for sequences have not yet been implemented\"))\n",
    "                                dataClassLabels=self.getDataLabels(currData, att)\n",
    "                                if classLabels.get(att) is None: #If it is the first step\n",
    "                                    classLabels[att]=dataClassLabels\n",
    "                                else:\n",
    "                                    #print(np.concatenate(dataClassLabels,classLabels[datclass]))\n",
    "                                    #This line is probably taking up all of the time...\n",
    "                                    if dataClassLabels not in classLabels[att]:\n",
    "                                        #print(dataClassLabels)\n",
    "                                        classLabels[att]=np.unique(np.concatenate([dataClassLabels, classLabels[att]]))\n",
    "                            else:\n",
    "                                if self.isSequence[att]!=True:\n",
    "                                    #If is it nominal and not a sequence\n",
    "                                    raise(NotImplementedError(\"Non-nominal data types for non-sequences have not yet been implemented\"))\n",
    "                                else:\n",
    "                                    #Add to the variable running sum\n",
    "                                    tempData = currData[0]\n",
    "                                    #print(tempData.keys)\n",
    "                                    variableSums[att] += sum(tempData[att])\n",
    "                        else:\n",
    "                            #Handle the derived variables\n",
    "                            currentDerivedData = self.deriveData(att, currData[0])\n",
    "                            variableSums[att] += sum(currentDerivedData)\n",
    "\n",
    "                    numDataPoints=numDataPoints+1\n",
    "                except:\n",
    "                    moreData=False\n",
    "                    print(\"Stopped at \" + str(numDataPoints) + \" data points\")\n",
    "            \n",
    "            oneHotEncoders={}\n",
    "            encodingLengths={}\n",
    "            dataDim=0\n",
    "            for att in attributes:\n",
    "                if self.isSequence[att]==False:\n",
    "                    oneHotEncoders[att]=self.buildEncoder(classLabels[att])\n",
    "                    encodingLengths[att]=classLabels[att].size\n",
    "                    #dataDim=dataDim+encodingLengths[datclass]\n",
    "                else:\n",
    "                    if self.isNominal[att]:\n",
    "                        raise(NotImplementedError(\"Nominal data types for sequences have not yet been implemented\"))\n",
    "                    else:\n",
    "                        encodingLengths[att]=1\n",
    "                        #dataDim=dataDim+1\n",
    "            print(\"Getting data indices\")\n",
    "            dataPointIndices=jsonReader.getDataIndices(self.dataFileName)\n",
    "            \n",
    "            #Set all of the summary information to self properties\n",
    "            self.computeMeansStandardDeviations(variableSums, numDataPoints, attributes)\n",
    "            self.numDataPoints=numDataPoints\n",
    "            self.encodingLengths=encodingLengths#A dictionary that maps attributes to the lengths of their vector encoding schemes\n",
    "            self.oneHotEncoders=oneHotEncoders#A dictionary of dictionaries where the outer dictionary maps attributes to encoding schemes and where each encoding scheme is a dictionary that maps attribute values to one hot encodings\n",
    "            #self.dataDim=dataDim#The sum of all the encoding lengths for the relevant attributes\n",
    "            #self.isSequence=#A dictionary that returns whether an attribute takes the form of a sequence of data\n",
    "            #self.isNominal=#A dictionary that returns whether an attribute is nominal in form (neither numeric nor ordinal)\n",
    "            self.dataPointIndices=dataPointIndices\n",
    "            \n",
    "            #Save that summary file so that it can be used next time\n",
    "            self.writeSummaryFile()\n",
    "        self.MetaDataLoaded=True \n",
    "        \n",
    "    def computeMeansStandardDeviations(self, varSums, numDataPoints, attributes):\n",
    "        print(\"Computing variable means and standard deviations\")\n",
    "        \n",
    "        numSequencePoints = numDataPoints*500\n",
    "        variableMeans = {}\n",
    "        for key in varSums:\n",
    "            variableMeans[key] = varSums[key]/numSequencePoints\n",
    "        \n",
    "        varResidualSums = {'altitude':0, 'gender':0, 'heart_rate':0, 'id':0, 'latitude':0, 'longitude':0,\n",
    "                   'speed':0, 'sport':0, 'timestamp':0, 'url':0, 'userId':0, 'time_elapsed': 0,\n",
    "                            'distance':0, 'new_workout':0, 'derived_speed':0}\n",
    "        \n",
    "        self.createSequentialGenerator()\n",
    "        moreData=True\n",
    "        countDP=0\n",
    "        while moreData:\n",
    "            if countDP%10000==0:\n",
    "                print(\"Currently at data point \" + str(countDP))\n",
    "            tryToggle = 0 #To get around suppressing any errors in all the try code, just try a single line and use this variable to trigger the rest\n",
    "            try:\n",
    "                currData=[self.getNextDataPointSequential()]\n",
    "                tryToggle = 1\n",
    "            except:\n",
    "                moreData=False\n",
    "                print(\"Stopped at \" + str(countDP) + \" data points\")\n",
    "            #dataClasses = self.getDataClasses(currData)#This could be removed to make it more effecient\n",
    "            if tryToggle == 1:\n",
    "                for att in attributes:\n",
    "                    if self.isNominal[att] != True: #If it is nominal data\n",
    "                        if self.isSequence[att]!=True:\n",
    "                            #If is it nominal and not a sequence\n",
    "                            raise(NotImplementedError(\"Non-nominal data types for non-sequences have not yet been implemented\"))\n",
    "                        else:\n",
    "                            if self.isDerived[att] == True: #Handle derived variables\n",
    "                                dataPointArray = np.array(self.deriveData(att, currData[0]))\n",
    "                            else:\n",
    "                                tempData = currData[0]\n",
    "                                dataPointArray = np.array(tempData[att])\n",
    "                            #Add to the variable running sum of squared residuals\n",
    "                            diff=np.subtract(dataPointArray, variableMeans[att])#Line is split for debugging\n",
    "                            sq=np.square(diff)\n",
    "                            varResidualSums[att] += np.sum(sq)\n",
    "                            #varResidualSums[att] += np.sum(np.square(np.subtract(dataPointArray, variableMeans[att])))\n",
    "                countDP += 1\n",
    "\n",
    "            #except:\n",
    "            #    moreData=False\n",
    "            #    print(\"Stopped at \" + str(countDP) + \" data points\")\n",
    "        \n",
    "        variableStds = {}\n",
    "        for key in varResidualSums:\n",
    "            variableStds[key] = np.sqrt(varResidualSums[key]/numSequencePoints)\n",
    "            \n",
    "        self.variableMeans = variableMeans\n",
    "        self.variableStds = variableStds\n",
    "        \n",
    "        \n",
    "    def oneHot(self, dataPoint, att):\n",
    "        #Takes the current data point and the attribute type and uses the data schema to provide the one-hot encoding for the variable\n",
    "        dataValue=dataPoint[att]       \n",
    "        #Use a stored schema dictionary to return the correct encoding scheme for the attribute (an encoding scheme is also a dictionary)\n",
    "        encoder=self.oneHotEncoders[att]\n",
    "        #Use this encoding scheme to get the encoding\n",
    "        encoding=encoder[dataValue]\n",
    "        return encoding\n",
    "    \n",
    "    def invertOneHots(self):\n",
    "        #This function assumes that all values in a given encoder are unique\n",
    "        inverseOneHotEncoders={}\n",
    "        for i, att in enumerate(self.attributes):\n",
    "            if self.isSequence[att]==False:\n",
    "                currentEncoder=self.oneHotEncoders[att]\n",
    "                inverseOneHotEncoders[att] = {str(v): k for k, v in currentEncoder.iteritems()}\n",
    "        return inverseOneHotEncoders\n",
    "\n",
    "    def getInputDim(self, targetAtt):\n",
    "        return self.dataDim - self.encodingLengths[targetAtt]\n",
    "\n",
    "    def getTargetDim(self, targetAtt):\n",
    "        return self.encodingLengths[targetAtt]\n",
    "    \n",
    "    \n",
    "class metaDataEndomondo(object):\n",
    "    #For disk storage of metadata\n",
    "    #Meant to be pickled and unpickled\n",
    "    def __init__(self, numDataPoints, encodingLengths, oneHotEncoders, isSequence, isNominal, isDerived, dataPointIndices,\n",
    "                 variableMeans, variableStds):\n",
    "        self.numDataPoints=numDataPoints\n",
    "        self.encodingLengths=encodingLengths\n",
    "        self.oneHotEncoders=oneHotEncoders\n",
    "        #self.dataDim=dataDim\n",
    "        self.isSequence=isSequence\n",
    "        self.isNominal=isNominal\n",
    "        self.isDerived=isDerived\n",
    "        self.dataPointIndices=dataPointIndices\n",
    "        self.variableMeans = variableMeans\n",
    "        self.variableStds = variableStds\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata\n",
      "Metadata loaded\n"
     ]
    }
   ],
   "source": [
    "endoReader=dataInterpreter(fn=\"../multimodalDBM/endomondoHR_proper.json\", allowMissingData=True, scaleVals=False)\n",
    "endoReader.buildDataSchema([\"sport\", \"heart_rate\",\"gender\", \"altitude\", \"time_elapsed\", \"distance\"], \"heart_rate\")\n",
    "batch_size=2\n",
    "num_steps=5\n",
    "trainValidTest='train'\n",
    "targetAtt='heart_rate'\n",
    "endoIter = endoReader.endoIteratorSupervised(batch_size, num_steps, trainValidTest)\n",
    "(inputs, targets) = endoIter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180656"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endoReader.numDataPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 34,\n",
       " 35,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 112,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 152,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 274,\n",
       " 295,\n",
       " 296,\n",
       " 298,\n",
       " 300,\n",
       " 304,\n",
       " 325,\n",
       " 357,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 370,\n",
       " 372,\n",
       " 375,\n",
       " 398,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 424,\n",
       " 430,\n",
       " 432,\n",
       " 433,\n",
       " 435,\n",
       " 438,\n",
       " 441,\n",
       " 444,\n",
       " 445,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 452,\n",
       " 454,\n",
       " 457,\n",
       " 458,\n",
       " 461,\n",
       " 463,\n",
       " 465,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 473,\n",
       " 474,\n",
       " 476,\n",
       " 478,\n",
       " 480,\n",
       " 482,\n",
       " 484,\n",
       " 486,\n",
       " 489,\n",
       " 491,\n",
       " 494,\n",
       " 496,\n",
       " 497,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 611,\n",
       " 612,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 621,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 656,\n",
       " 657,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 679,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 694,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 726,\n",
       " 730,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 752,\n",
       " 754,\n",
       " 761,\n",
       " 762,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 776,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 787,\n",
       " 793,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 799,\n",
       " 802,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 825,\n",
       " 829,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 838,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 911,\n",
       " 913,\n",
       " 915,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 932,\n",
       " 933,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 942,\n",
       " 944,\n",
       " 945,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 961,\n",
       " 962,\n",
       " 964,\n",
       " 965,\n",
       " 967,\n",
       " 972,\n",
       " 983,\n",
       " 984,\n",
       " 987,\n",
       " 990,\n",
       " 1010,\n",
       " 1011,\n",
       " 1032,\n",
       " 1033,\n",
       " 1042,\n",
       " 1054,\n",
       " 1056,\n",
       " 1058,\n",
       " 1068,\n",
       " 1073,\n",
       " 1074,\n",
       " 1075,\n",
       " 1076,\n",
       " 1077,\n",
       " 1078,\n",
       " 1079,\n",
       " 1080,\n",
       " 1081,\n",
       " 1082,\n",
       " 1083,\n",
       " 1084,\n",
       " 1085,\n",
       " 1086,\n",
       " 1087,\n",
       " 1088,\n",
       " 1089,\n",
       " 1090,\n",
       " 1091,\n",
       " 1092,\n",
       " 1093,\n",
       " 1094,\n",
       " 1095,\n",
       " 1096,\n",
       " 1097,\n",
       " 1098,\n",
       " 1099,\n",
       " 1100,\n",
       " 1101,\n",
       " 1102,\n",
       " 1103,\n",
       " 1104,\n",
       " 1105,\n",
       " 1106,\n",
       " 1107,\n",
       " 1108,\n",
       " 1109,\n",
       " 1111,\n",
       " 1112,\n",
       " 1113,\n",
       " 1114,\n",
       " 1116,\n",
       " 1117,\n",
       " 1119,\n",
       " 1120,\n",
       " 1121,\n",
       " 1122,\n",
       " 1124,\n",
       " 1125,\n",
       " 1126,\n",
       " 1127,\n",
       " 1128,\n",
       " 1129,\n",
       " 1130,\n",
       " 1131,\n",
       " 1132,\n",
       " 1133,\n",
       " 1134,\n",
       " 1135,\n",
       " 1136,\n",
       " 1138,\n",
       " 1139,\n",
       " 1140,\n",
       " 1141,\n",
       " 1142,\n",
       " 1143,\n",
       " 1144,\n",
       " 1145,\n",
       " 1146,\n",
       " 1147,\n",
       " 1148,\n",
       " 1149,\n",
       " 1150,\n",
       " 1151,\n",
       " 1152,\n",
       " 1153,\n",
       " 1155,\n",
       " 1156,\n",
       " 1157,\n",
       " 1159,\n",
       " 1160,\n",
       " 1161,\n",
       " 1163,\n",
       " 1164,\n",
       " 1165,\n",
       " 1166,\n",
       " 1167,\n",
       " 1168,\n",
       " 1169,\n",
       " 1170,\n",
       " 1171,\n",
       " 1172,\n",
       " 1173,\n",
       " 1174,\n",
       " 1175,\n",
       " 1176,\n",
       " 1178,\n",
       " 1179,\n",
       " 1180,\n",
       " 1182,\n",
       " 1183,\n",
       " 1184,\n",
       " 1185,\n",
       " 1186,\n",
       " 1187,\n",
       " 1188,\n",
       " 1189,\n",
       " 1190,\n",
       " 1191,\n",
       " 1192,\n",
       " 1193,\n",
       " 1194,\n",
       " 1195,\n",
       " 1196,\n",
       " 1197,\n",
       " 1198,\n",
       " 1199,\n",
       " 1200,\n",
       " 1201,\n",
       " 1202,\n",
       " 1203,\n",
       " 1204,\n",
       " 1205,\n",
       " 1206,\n",
       " 1207,\n",
       " 1208,\n",
       " 1209,\n",
       " 1210,\n",
       " 1211,\n",
       " 1212,\n",
       " 1213,\n",
       " 1214,\n",
       " 1215,\n",
       " 1216,\n",
       " 1217,\n",
       " 1218,\n",
       " 1219,\n",
       " 1220,\n",
       " 1221,\n",
       " 1222,\n",
       " 1223,\n",
       " 1224,\n",
       " 1225,\n",
       " 1226,\n",
       " 1229,\n",
       " 1230,\n",
       " 1231,\n",
       " 1232,\n",
       " 1233,\n",
       " 1234,\n",
       " 1235,\n",
       " 1236,\n",
       " 1237,\n",
       " 1238,\n",
       " 1239,\n",
       " 1240,\n",
       " 1241,\n",
       " 1242,\n",
       " 1243,\n",
       " 1244,\n",
       " 1245,\n",
       " 1246,\n",
       " 1247,\n",
       " 1248,\n",
       " 1249,\n",
       " 1251,\n",
       " 1252,\n",
       " 1253,\n",
       " 1254,\n",
       " 1255,\n",
       " 1256,\n",
       " 1257,\n",
       " 1258,\n",
       " 1260,\n",
       " 1261,\n",
       " 1262,\n",
       " 1263,\n",
       " 1264,\n",
       " 1265,\n",
       " 1266,\n",
       " 1267,\n",
       " 1269,\n",
       " 1270,\n",
       " 1271,\n",
       " 1272,\n",
       " 1273,\n",
       " 1274,\n",
       " 1275,\n",
       " 1276,\n",
       " 1277,\n",
       " 1278,\n",
       " 1279,\n",
       " 1280,\n",
       " 1281,\n",
       " 1282,\n",
       " 1283,\n",
       " 1284,\n",
       " 1285,\n",
       " 1286,\n",
       " 1287,\n",
       " 1288,\n",
       " 1289,\n",
       " 1290,\n",
       " 1291,\n",
       " 1292,\n",
       " 1293,\n",
       " 1294,\n",
       " 1295,\n",
       " 1296,\n",
       " 1297,\n",
       " 1299,\n",
       " 1300,\n",
       " 1301,\n",
       " 1302,\n",
       " 1303,\n",
       " 1304,\n",
       " 1305,\n",
       " 1306,\n",
       " 1307,\n",
       " 1308,\n",
       " 1309,\n",
       " 1310,\n",
       " 1311,\n",
       " 1312,\n",
       " 1313,\n",
       " 1314,\n",
       " 1315,\n",
       " 1316,\n",
       " 1317,\n",
       " 1318,\n",
       " 1319,\n",
       " 1320,\n",
       " 1321,\n",
       " 1322,\n",
       " 1323,\n",
       " 1324,\n",
       " 1325,\n",
       " 1326,\n",
       " 1327,\n",
       " 1328,\n",
       " 1329,\n",
       " 1330,\n",
       " 1331,\n",
       " 1332,\n",
       " 1333,\n",
       " 1334,\n",
       " 1335,\n",
       " 1337,\n",
       " 1338,\n",
       " 1339,\n",
       " 1340,\n",
       " 1341,\n",
       " 1342,\n",
       " 1343,\n",
       " 1344,\n",
       " 1345,\n",
       " 1346,\n",
       " 1347,\n",
       " 1348,\n",
       " 1349,\n",
       " 1350,\n",
       " 1351,\n",
       " 1352,\n",
       " 1353,\n",
       " 1354,\n",
       " 1355,\n",
       " 1356,\n",
       " 1357,\n",
       " 1358,\n",
       " 1359,\n",
       " 1360,\n",
       " 1361,\n",
       " 1362,\n",
       " 1363,\n",
       " 1364,\n",
       " 1365,\n",
       " 1366,\n",
       " 1367,\n",
       " 1368,\n",
       " 1369,\n",
       " 1370,\n",
       " 1371,\n",
       " 1372,\n",
       " 1373,\n",
       " 1374,\n",
       " 1375,\n",
       " 1376,\n",
       " 1377,\n",
       " 1378,\n",
       " 1379,\n",
       " 1380,\n",
       " 1381,\n",
       " 1382,\n",
       " 1383,\n",
       " 1384,\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endoReader.dataPointList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
